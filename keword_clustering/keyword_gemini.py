# -*- coding: utf-8 -*-
import sys
from pathlib import Path
import pandas as pd
from tqdm import tqdm
import numpy as np
import google.generativeai as genai
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
import time
import json
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from gensim.corpora import Dictionary

tqdm.pandas()

# 1. ì‚¬ìš©ì ì •ì˜ í•„í„° ì„¤ì • --------------------------------------------------------
custom_nouns = [
    'ëŒ€í†µë ¹ë¹„ì„œì‹¤', 'êµ­ê°€ì•ˆë³´ì‹¤', 'ëŒ€í†µë ¹ê²½í˜¸ì²˜', 'í—Œë²•ìƒëŒ€í†µë ¹ìë¬¸ê¸°êµ¬', 'êµ­ê°€ì•ˆì „ë³´ì¥íšŒì˜',
    'ë¯¼ì£¼í‰í™”í†µì¼ìë¬¸íšŒì˜', 'êµ­ë¯¼ê²½ì œìë¬¸íšŒì˜', 'êµ­ê°€ê³¼í•™ê¸°ìˆ ìë¬¸íšŒì˜', 'ê°ì‚¬ì›', 'êµ­ê°€ì •ë³´ì›',
    'ë°©ì†¡í†µì‹ ìœ„ì›íšŒ', 'íŠ¹ë³„ê°ì°°ê´€', 'ê³ ìœ„ê³µì§ìë²”ì£„ìˆ˜ì‚¬ì²˜', 'êµ­ê°€ì¸ê¶Œìœ„ì›íšŒ', 'êµ­ë¬´ì¡°ì •ì‹¤',
    'êµ­ë¬´ì´ë¦¬ë¹„ì„œì‹¤', 'ì¸ì‚¬í˜ì‹ ì²˜', 'ë²•ì œì²˜', 'ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜', 'ê³µì •ê±°ë˜ìœ„ì›íšŒ',
    'êµ­ë¯¼ê¶Œìµìœ„ì›íšŒ', 'ê¸ˆìœµìœ„ì›íšŒ', 'ê°œì¸ì •ë³´ë³´í˜¸ìœ„ì›íšŒ', 'ì›ìë ¥ì•ˆì „ìœ„ì›íšŒ', 'ê¸°íšì¬ì •ë¶€',
    'êµ­ì„¸ì²­', 'ê´€ì„¸ì²­', 'ì¡°ë‹¬ì²­', 'í†µê³„ì²­', 'êµìœ¡ë¶€', 'ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€', 'ìš°ì£¼í•­ê³µì²­',
    'ì™¸êµë¶€', 'ì¬ì™¸ë™í¬ì²­', 'í†µì¼ë¶€', 'ë²•ë¬´ë¶€', 'ê²€ì°°ì²­', 'êµ­ë°©ë¶€', 'ë³‘ë¬´ì²­', 'ë°©ìœ„ì‚¬ì—…ì²­',
    'í–‰ì •ì•ˆì „ë¶€', 'ê²½ì°°ì²­', 'ì†Œë°©ì²­', 'êµ­ê°€ë³´í›ˆë¶€', 'ë¬¸í™”ì²´ìœ¡ê´€ê´‘ë¶€', 'êµ­ê°€ìœ ì‚°ì²­',
    'ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€', 'ë†ì´Œì§„í¥ì²­', 'ì‚°ë¦¼ì²­', 'ì‚°ì—…í†µìƒìì›ë¶€', 'íŠ¹í—ˆì²­', 'ë³´ê±´ë³µì§€ë¶€',
    'ì§ˆë³‘ê´€ë¦¬ì²­', 'í™˜ê²½ë¶€', 'ê¸°ìƒì²­', 'ê³ ìš©ë…¸ë™ë¶€', 'ì—¬ì„±ê°€ì¡±ë¶€', 'êµ­í† êµí†µë¶€',
    'í–‰ì •ì¤‘ì‹¬ë³µí•©ë„ì‹œê±´ì„¤ì²­', 'ìƒˆë§Œê¸ˆê°œë°œì²­', 'í•´ì–‘ìˆ˜ì‚°ë¶€', 'í•´ì–‘ê²½ì°°ì²­', 'ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€',
    'ìƒì„ìœ„ì›íšŒ', 'ë²•ì œì‚¬ë²•ìœ„ì›íšŒ', 'ì •ë¬´ìœ„ì›íšŒ', 'ê¸°íšì¬ì •ìœ„ì›íšŒ', 'êµìœ¡ìœ„ì›íšŒ',
    'ê³¼í•™ê¸°ìˆ ì •ë³´ë°©ì†¡í†µì‹ ìœ„ì›íšŒ', 'ì™¸êµí†µì¼ìœ„ì›íšŒ', 'êµ­ë°©ìœ„ì›íšŒ', 'í–‰ì •ì•ˆì „ìœ„ì›íšŒ',
    'ë¬¸í™”ì²´ìœ¡ê´€ê´‘ìœ„ì›íšŒ', 'ë†ë¦¼ì¶•ì‚°ì‹í’ˆí•´ì–‘ìˆ˜ì‚°ìœ„ì›íšŒ', 'ì‚°ì—…í†µìƒìì›ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ìœ„ì›íšŒ',
    'ë³´ê±´ë³µì§€ìœ„ì›íšŒ', 'í™˜ê²½ë…¸ë™ìœ„ì›íšŒ', 'êµ­í† êµí†µìœ„ì›íšŒ', 'ì •ë³´ìœ„ì›íšŒ', 'ì—¬ì„±ê°€ì¡±ìœ„ì›íšŒ',
    'ì˜ˆì‚°ê²°ì‚°íŠ¹ë³„ìœ„ì›íšŒ', 'íŠ¹ë³„ìœ„ì›íšŒ', 'ì†Œìœ„ì›íšŒ', 'ë²•ì•ˆì‹¬ì‚¬ì†Œìœ„', 'ì˜ì•ˆ', 'ë²•ë¥ ì•ˆ',
    'ì˜ˆì‚°ì•ˆ', 'ë™ì˜ì•ˆ', 'ìŠ¹ì¸ì•ˆ', 'ê²°ì˜ì•ˆ', 'ê±´ì˜ì•ˆ', 'ê·œì¹™ì•ˆ', 'ì„ ì¶œì•ˆ', 'ë°œì˜', 'ì œì¶œ',
    'ì œì•ˆ', 'ì œì˜', 'ì˜ê²°', 'ë¶€ê²°', 'íê¸°', 'ê°€ê²°', 'ì±„íƒ', 'ì…ë²•ì˜ˆê³ ', 'ê³µí¬', 'ì‹œí–‰',
    'ê°œì •', 'ì œì •', 'íì§€', 'ì¼ë¶€ê°œì •', 'ì „ë¶€ê°œì •',
    'í—Œë²•', 'ë¯¼ë²•', 'í˜•ë²•', 'ìƒë²•', 'í–‰ì •ë²•', 'ë…¸ë™ë²•', 'ì„¸ë²•', 'í™˜ê²½ë²•', 'ì •ë³´í†µì‹ ë²•',
    'ê¸ˆìœµë²•', 'ë³´ê±´ì˜ë£Œë²•', 'êµìœ¡ë²•', 'ë¬¸í™”ì˜ˆìˆ ë²•', 'ë†ë¦¼ë²•', 'ê±´ì„¤ë²•', 'í•´ì–‘ë²•', 'ê¹€ì˜ë€ë²•',
    'ë¶€ì •ì²­íƒê¸ˆì§€ë²•', 'ê³µì§ììœ¤ë¦¬ë²•', 'ì •ì¹˜ìê¸ˆë²•', 'ê³µì§ì„ ê±°ë²•', 'ì „ê¸°í†µì‹ ì‚¬ì—…ë²•',
    'ê°œì¸ì •ë³´ë³´í˜¸ë²•', 'êµ­ê°€ìœ ì‚°ìˆ˜ë¦¬ê¸°ìˆ ìœ„ì›íšŒ', 'ë¶€ê°€ê°€ì¹˜ì„¸ë²•', 'ìˆ˜ì…ì‹í’ˆì•ˆì „ê´€ë¦¬íŠ¹ë³„ë²•',
    'ë‹¤ë¬¸í™”ê°€ì¡±ì§€ì›ë²•',
    'ì¸ê³µì§€ëŠ¥', 'ë¹…ë°ì´í„°', 'ì‚¬ë¬¼ì¸í„°ë„·', 'í´ë¼ìš°ë“œ', 'ë¸”ë¡ì²´ì¸', 'ë©”íƒ€ë²„ìŠ¤', 'ë””ì§€í„¸í”Œë«í¼',
    'ì „ìì •ë¶€', 'ë””ì§€í„¸ì „í™˜', 'ì‚¬ì´ë²„ë³´ì•ˆ', 'ë””ì§€í„¸ë‰´ë”œ', 'ìŠ¤ë§ˆíŠ¸ì‹œí‹°', 'ë””ì§€í„¸í¬ìš©',
    'ì˜¨ë¼ì¸í”Œë«í¼', 'ì „ììƒê±°ë˜',
    'ì½”ë¡œë‚˜19', 'ê°ì—¼ë³‘', 'ë°±ì‹ ', 'ë°©ì—­', 'ì‚¬íšŒì ê±°ë¦¬ë‘ê¸°', 'ì¬ë‚œì§€ì›ê¸ˆ', 'ê¸°í›„ë³€í™”',
    'ë¯¸ì„¸ë¨¼ì§€', 'íê¸°ë¬¼ì²˜ë¦¬', 'ì¬í™œìš©', 'ìˆœí™˜ê²½ì œ', 'ì  ë”í‰ë“±', 'ì„±í¬ë¡±', 'ì„±í­ë ¥', 'ìŠ¤í† í‚¹',
    'ê°€ì •í­ë ¥', 'ë””ì§€í„¸ì„±ë²”ì£„', 'ì²­ë…„ì •ì±…', 'ì²­ë…„ê³ ìš©', 'ì²­ë…„ì£¼íƒ', 'í•™ìê¸ˆëŒ€ì¶œ', 'êµìœ¡ê²©ì°¨',
]

initial_stopwords = frozenset({
    'ì¡°', 'í•­', 'í˜¸', 'ê²½ìš°', 'ë“±', 'ìˆ˜', 'ê²ƒ', 'ì´', 'ì°¨', 'í›„', 'ì´ìƒ', 'ì´í•˜', 'ì´ë‚´',
    'ì•ˆ', 'ì†Œ', 'ëŒ€', 'ì ', 'ê°„', 'ê³³', 'í•´ë‹¹', 'ì™¸', 'ë‚˜', 'ë°”', 'ì‹œ', 'ê´€ë ¨', 'ê´€í•˜ì—¬',
    'ëŒ€í•˜ì—¬', 'ë”°ë¼', 'ë”°ë¥¸', 'ìœ„í•˜ì—¬', 'ì˜í•˜ì—¬', 'ë•Œ', 'ê°', 'ì', 'ì¸', 'ë‚´', 'ì¤‘',
    'ë•Œë¬¸', 'ìœ„í•´', 'í†µí•´', 'ë¶€í„°', 'ê¹Œì§€', 'ë™ì•ˆ', 'ì‚¬ì´', 'ê¸°ì¤€', 'ë³„ë„', 'ë³„ì²¨', 'ë³„í‘œ',
    'ì œí•œ', 'íŠ¹ì¹™', 'ê°€ëŠ¥', 'ê³¼ì •', 'ê¸°ë°˜', 'ê¸°ì¡´', 'ê·¼ê±°', 'ê¸°ëŠ¥', 'ë°©ì‹', 'ë²”ìœ„', 'ì‚¬í•­',
    'ì‹œì ', 'ìµœê·¼', 'ë…„', 'ì¥', 'í•´', 'ëª…', 'ë‚ ', 'íšŒ', 'ë™', 'ë°', 'êµ­', 'ë°–', 'ì†', 'ì‹',
    'ê·œ', 'í˜„í–‰ë²•', 'ì§', 'ë²”', 'ë§Œ', 'ì…', 'ì‹ ',
})

initial_excluded_terms = frozenset({
    'ì£¼ìš”', 'ìˆ˜ì‚¬', 'ê´€ë ¨', 'ì‚¬í•­', 'ì •ì±…', 'ëŒ€ìƒ', 'ë°©ì•ˆ', 'ì¶”ì§„', 'ê°•í™”', 'ê°œì„ ', 'ì§€ì›',
    'í™•ëŒ€', 'ì¡°ì¹˜', 'í•„ìš”', 'í˜„í™©', 'ê¸°ë°˜', 'ê³¼ì •', 'ê¸°ì¡´', 'ê·¼ê±°', 'ê¸°ëŠ¥', 'ë°©ì‹', 'ë²”ìœ„',
    'í™œë™', 'ìš´ì˜', 'ê´€ë¦¬', 'ì‹¤ì‹œ', 'í™•ë³´', 'êµ¬ì„±', 'ì„¤ì¹˜', 'ì§€ì •', 'ê³„íš', 'ìˆ˜ë¦½',
})

preserve_terms = frozenset({'ë²•ë¥ ', 'ë²•ì•ˆ', 'ì…ë²•', 'ê°œì •', 'ì œì •', 'ì‹œí–‰', 'ê³µí¬', 'íì§€', 'ì¡°ë¡€', 'ê·œì •', 'ì¡°í•­', 'ì˜ê²°'})
excluded_bigrams = frozenset({'êµìœ¡ ì‹¤ì‹œ', 'ì§•ì—­ ë²Œê¸ˆ', 'ìˆ˜ë¦½ ì‹œí–‰', 'ìš´ì˜ ê´€ë¦¬'})

# 2. ë²•ë ¹ êµ¬ì¡° íŒ¨í„´ ì œê±° í•¨ìˆ˜ -----------------------------------------------------
def remove_law_structure_phrases(text):
    patterns = [
        r'ì œ\d+ì¡°ì˜?\d*(?:ì œ\d+í•­)?(?:ì œ\d+í˜¸)?',
        r'ì•ˆ\s*ì œ\d+ì¡°ì˜?\d*(?:ì œ\d+í•­)?(?:ì œ\d+í˜¸)?',
        r'\d+ë…„\s*\d+ì›”\s*\d+ì¼',
        r'\d+ë§Œ\s*\d+ì²œ?\s*\d+ëª…',
        r'\d+%',
        r'\b(?:ëˆ„êµ¬ë‚˜|ì§€ë‹ˆê³ |ìœ ì‚¬í•œ|ê¸°ì¤€|ì•½)\b',
        r'ì‹ ì„¤', r'ì •ë¹„', r'ì¡°ì •', r'ì¸ìš©ì¡°ë¬¸', r'ì •ë¹„\s*\(.*?\)', r'ì•ˆ'
    ]
    combined = '|'.join(patterns)
    text = re.sub(combined, ' ', text)
    return re.sub(r'\s+', ' ', text).strip()

# 3. Gemini ì„ë² ë”© ìƒì„± í•¨ìˆ˜ -----------------------------------------------------
def get_gemini_embeddings(texts, model_name="gemini-embedding-exp-03-07", task_type="CLUSTERING", max_workers=1):
    embeddings = []
    def embed_one(text):
        try:
            response = genai.embed_content(
                model=model_name,
                content=text,
                task_type=task_type
            )
            return response['embedding']
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ì˜¤ë¥˜: {str(e)[:100]}")
            return None
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(embed_one, text) for text in texts]
        for future in tqdm(as_completed(futures), total=len(futures), desc="Gemini ì„ë² ë”© ìƒì„±"):
            if (emb := future.result()) is not None:
                embeddings.append(emb)
    
    print(f"âœ… ìƒì„±ëœ ì„ë² ë”© ìˆ˜: {len(embeddings)}")
    return embeddings

# 4. Gemini ì „ì²˜ë¦¬ í•¨ìˆ˜ ---------------------------------------------------------
def gemini_tokenize_and_filter(text, model):
    prompt = f"""
[í•œêµ­ì–´ ì§€ì‹œì‚¬í•­]
ì•„ë˜ ë²•ì•ˆ í…ìŠ¤íŠ¸ì—ì„œ
1. ì‚¬ìš©ì ì •ì˜ ëª…ì‚¬ {custom_nouns}ëŠ” ë°˜ë“œì‹œ ë³´ì¡´
2. ì´ˆê¸° ë¶ˆìš©ì–´ {list(initial_stopwords)}ì™€ ì œì™¸ ë‹¨ì–´ {list(initial_excluded_terms)}ëŠ” ì œê±°
3. ì œì™¸ ë°”ì´ê·¸ë¨ {list(excluded_bigrams)}ëŠ” ì „ì²´ ì‚­ì œ
4. ë³´ì¡´ ìš©ì–´ {list(preserve_terms)}ëŠ” ë¬´ì¡°ê±´ ìœ ì§€
5. ê²°ê³¼ë¥¼ ê³µë°± êµ¬ë¶„ ë¬¸ìì—´ë¡œ ë°˜í™˜

í…ìŠ¤íŠ¸:
{text[:2000]}
"""
    for _ in range(3):
        try:
            response = model.generate_content(prompt)
            tokens = response.text.strip()
            if tokens and len(tokens) > 1:
                return tokens
        except Exception:
            time.sleep(1)
    return ""

def parallel_gemini_tokenize_and_filter(texts, model, max_workers=1):
    results = [None] * len(texts)
    def process_one(i, t):
        return (i, gemini_tokenize_and_filter(t, model))
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_one, i, t) for i, t in enumerate(texts)]
        for f in tqdm(as_completed(futures), total=len(futures), desc="Gemini ë³‘ë ¬ ì „ì²˜ë¦¬"):
            i, tokens = f.result()
            results[i] = tokens
    
    return results

# 5. LDA ê¸°ë°˜ ìµœì  êµ°ì§‘ìˆ˜ íƒìƒ‰ ---------------------------------------------------
def find_optimal_n_topics_lda_fast(X, texts, dictionary, corpus, vectorizer):
    from sklearn.decomposition import LatentDirichletAllocation
    from sklearn.metrics import silhouette_score

    # 1ë‹¨ê³„: ëŒ€ëµì  ë²”ìœ„ íƒìƒ‰
    n_range_coarse = range(10, 201, 50)
    best_score_coarse = -np.inf
    best_n_coarse = 10

    for n_topics in tqdm(n_range_coarse, desc="1ë‹¨ê³„: ëŒ€ëµì  êµ°ì§‘ìˆ˜ íƒìƒ‰"):
        lda = LatentDirichletAllocation(
            n_components=n_topics,
            learning_method='online',
            batch_size=256,
            max_iter=15,
            random_state=42,
            n_jobs=1
        )
        topic_dist = lda.fit_transform(X)
        silhouette = silhouette_score(X, topic_dist.argmax(axis=1))
        if silhouette > best_score_coarse:
            best_score_coarse = silhouette
            best_n_coarse = n_topics

    # 2ë‹¨ê³„: ìƒì„¸ ë²”ìœ„ íƒìƒ‰
    start = max(10, best_n_coarse - 40)
    end = min(200, best_n_coarse + 40)
    n_range_fine = range(start, end+1, 10)

    def train_lda(n):
        lda = LatentDirichletAllocation(
            n_components=n,
            learning_method='online',
            batch_size=256,
            max_iter=15,
            random_state=42,
            n_jobs=1
        )
        lda.fit(X)
        return lda

    print("2ë‹¨ê³„: ìƒì„¸ êµ°ì§‘ìˆ˜ íƒìƒ‰ (ë³‘ë ¬)")
    with ThreadPoolExecutor(max_workers=1) as executor:
        futures = {executor.submit(train_lda, n): n for n in n_range_fine}
        models = {}
        for future in tqdm(as_completed(futures), total=len(futures)):
            n = futures[future]
            models[n] = future.result()

    # ìµœì  ëª¨ë¸ ì„ ì •
    best_score = -np.inf
    best_n = 10
    for n, model in models.items():
        topic_dist = model.transform(X)
        silhouette = silhouette_score(X, topic_dist.argmax(axis=1))
        if silhouette > best_score:
            best_score = silhouette
            best_n = n

    return best_n

# 6. Gemini ê¸°ë°˜ êµ°ì§‘ìˆ˜ ê²°ì • -----------------------------------------------------
def get_optimal_clusters_with_gemini(embeddings, sample_texts, model):
    prompt = f'''[í•œêµ­ì–´ ì§€ì‹œì‚¬í•­]
ìƒ˜í”Œ í…ìŠ¤íŠ¸: {sample_texts[:2000]}
ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ 1ê°œ ì¶”ì²œ (10~200):
{{"optimal_clusters": ì •ìˆ˜}}'''
    
    for _ in range(3):
        try:
            response = model.generate_content(prompt)
            result = json.loads(response.text.strip())
            if 10 <= (n := result["optimal_clusters"]) <= 200:
                return n
        except Exception:
            time.sleep(1)
    return None

# 7. í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œ ì¶”ì¶œ --------------------------------------------------------
def gemini_cluster_keywords(cluster_id, texts, model):
    prompt = f'''[í•œêµ­ì–´ ì§€ì‹œì‚¬í•­]
ë²•ì•ˆ ìƒ˜í”Œ: {texts[:2000]}
4ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ:
- ë²•ë¥  ì¡°ë¬¸/ì •ì±…ëª… ìš°ì„ 
- JSON ë°°ì—´ë¡œë§Œ ë°˜í™˜'''
    
    for _ in range(3):
        try:
            response = model.generate_content(prompt)
            if (match := re.search(r'\[".*?"(?:,\s*".*?")*\]', response.text)):
                return json.loads(match.group(0))[:4]
        except Exception as e:
            print(f"âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜ (í´ëŸ¬ìŠ¤í„° {cluster_id}): {str(e)}")
            time.sleep(1)
    
    # Fallback
    vectorizer = TfidfVectorizer(max_features=20)
    X = vectorizer.fit_transform([texts])
    return vectorizer.get_feature_names_out()[:4].tolist()

# 8. ë©”ì¸ ì‹¤í–‰ ë¡œì§ --------------------------------------------------------------
if __name__ == '__main__':
    # ì´ˆê¸°í™”
    GEMINI_API_KEY = "AIzaSyA8M00iSzCK1Lvc5YfxamYgQf-Lh4xh5R0"
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel('gemini-1.5-pro-latest')

    # ë°ì´í„° ë¡œë“œ
    file_path = Path(r"C:/Users/1-02/Desktop/DAMF2/laws-radar/geovote/data/bill_filtered_final.csv")
    df = pd.read_csv(file_path, encoding='utf-8-sig')

    # 1. ì „ì²˜ë¦¬
    print("ğŸ”„ ë²•ë ¹ êµ¬ì¡° íŒ¨í„´ ì œê±° ì¤‘...")
    df['content'] = df['content'].apply(remove_law_structure_phrases)

    print("ğŸ”„ Gemini ì „ì²˜ë¦¬ ì¤‘...")
    df['content'] = parallel_gemini_tokenize_and_filter(df['content'].tolist(), model, 1)

    # 2. ì„ë² ë”© (ìœ íš¨í•œ ë°ì´í„° í•„í„°ë§)
    print("ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")
    embeddings = get_gemini_embeddings(df['content'].tolist())
    
    # ìœ íš¨í•œ ì¸ë±ìŠ¤ ì¶”ì¶œ
    valid_indices = [i for i, emb in enumerate(embeddings) if emb is not None]
    filtered_df = df.iloc[valid_indices].copy()
    valid_embeddings = np.array([emb for emb in embeddings if emb is not None])

    # â–¼â–¼â–¼ ìˆ˜ì •ëœ ë¶€ë¶„: ë°°ì—´ í¬ê¸° í™•ì¸ â–¼â–¼â–¼
    if valid_embeddings.size == 0:
        print("âŒ ì„ë² ë”© ì‹¤íŒ¨: ìƒì„±ëœ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤")
        sys.exit(1)
    # â–²â–²â–²

    # 3. êµ°ì§‘ìˆ˜ ê²°ì •
    print("ğŸ” êµ°ì§‘ìˆ˜ ë¶„ì„ ì¤‘...")
    n_clusters = get_optimal_clusters_with_gemini(
        valid_embeddings, 
        ' '.join(filtered_df.sample(min(100, len(filtered_df)))['content']), 
        model
    )
    
    if not n_clusters:
        print("âš ï¸ LDA ë°©ì‹ìœ¼ë¡œ ì „í™˜")
        vectorizer = TfidfVectorizer(max_df=0.7, min_df=5, ngram_range=(1,3), max_features=5000)
        X = vectorizer.fit_transform(filtered_df['content'])
        texts = [doc.split() for doc in filtered_df['content']]
        n_clusters = find_optimal_n_topics_lda_fast(X, texts, Dictionary(texts), [], vectorizer)
    
    n_samples = len(valid_embeddings)
    n_clusters = min(n_clusters, n_samples)
    print(f"âœ… ìµœì¢… êµ°ì§‘ìˆ˜: {n_clusters} (ìƒ˜í”Œ ìˆ˜: {n_samples})")

    # 4. í´ëŸ¬ìŠ¤í„°ë§
    print("ğŸ”„ í´ëŸ¬ìŠ¤í„°ë§ ì¤‘...")
    filtered_df['topic'] = KMeans(n_clusters=n_clusters, random_state=42).fit_predict(valid_embeddings)

    # 5. ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì— ë³‘í•©
    df = df.merge(filtered_df[['topic']], how='left', left_index=True, right_index=True)
    df.rename(columns={'topic_y': 'topic'}, inplace=True)
    df['topic'] = df['topic'].fillna(-1).astype(int)  # ì„ë² ë”© ì‹¤íŒ¨ í–‰ì€ -1ë¡œ í‘œì‹œ

    # 6. í‚¤ì›Œë“œ ì¶”ì¶œ
    print("ğŸ”„ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
    cluster_texts = filtered_df.groupby('topic')['content'].apply(lambda x: ' '.join(x.sample(min(10, len(x)))))
    topic_labels = {}
    
    with ThreadPoolExecutor(1) as executor:
        futures = {executor.submit(gemini_cluster_keywords, cid, txt, model): cid 
                  for cid, txt in cluster_texts.items()}
        for future in tqdm(as_completed(futures), total=len(futures)):
            cid = futures[future]
            topic_labels[cid] = future.result()
            print(f"âœ… í´ëŸ¬ìŠ¤í„° {cid} í‚¤ì›Œë“œ: {topic_labels[cid]}")

    # 7. ê²°ê³¼ ë³‘í•©
    df['topic_label'] = df['topic'].map(lambda x: ', '.join(topic_labels.get(x, ['ê¸°íƒ€'])))

    # 8. ì €ì¥
    output_path = Path('data/bill_gemini_clustering.csv')
    df[['age', 'title', 'bill_id', 'bill_number', 'content', 'topic', 'topic_label']].to_csv(
        output_path, index=False, encoding='utf-8-sig'
    )
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")
