# -*- coding: utf-8 -*-
import sys
from pathlib import Path
import pandas as pd
from tqdm import tqdm
import numpy as np
import google.generativeai as genai
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
import time
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from gensim.corpora import Dictionary

tqdm.pandas()

# 1. Gemini ì„ë² ë”© ìƒì„± í•¨ìˆ˜
def get_gemini_embeddings(texts, model_name="models/embedding-001", task_type="CLUSTERING", max_workers=4):
    embeddings = []
    def embed_one(text):
        try:
            response = genai.embed_content(
                model=model_name,
                content=text,
                task_type=task_type
            )
            return response['embedding']
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ì˜¤ë¥˜: {str(e)[:100]}")
            return None
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(embed_one, text) for text in texts]
        for future in tqdm(as_completed(futures), total=len(futures), desc="Gemini ì„ë² ë”© ìƒì„±"):
            emb = future.result()
            if emb is not None:
                embeddings.append(emb)
    print(f"âœ… ìƒì„±ëœ ì„ë² ë”© ìˆ˜: {len(embeddings)}")
    return embeddings

# 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
def gemini_tokenize_and_filter(text, model):
    prompt = f"""
[í•œêµ­ì–´ ì§€ì‹œì‚¬í•­]
ì•„ë˜ ë²•ì•ˆ í…ìŠ¤íŠ¸ì—ì„œ
1. ì¼ë°˜ ë¶ˆìš©ì–´/ì¤‘ë³µë‹¨ì–´ ì œê±°
2. ë²•ë¥  ìš©ì–´ ì¤‘ì‹¬ í† í°í™”
3. ê²°ê³¼ë¥¼ ê³µë°± êµ¬ë¶„ ë¬¸ìì—´ë¡œ ë°˜í™˜
í…ìŠ¤íŠ¸:
{text[:2000]}
"""
    for _ in range(3):
        try:
            response = model.generate_content(prompt)
            tokens = response.text.strip()
            if tokens and len(tokens) > 1:
                return tokens
        except Exception:
            time.sleep(1)
    return ""

def parallel_gemini_tokenize_and_filter(texts, model, max_workers=4):
    results = [None] * len(texts)
    def process_one(i, t):
        return (i, gemini_tokenize_and_filter(t, model))
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_one, i, t) for i, t in enumerate(texts)]
        for f in tqdm(as_completed(futures), total=len(futures), desc="Gemini ë³‘ë ¬ ì „ì²˜ë¦¬"):
            i, tokens = f.result()
            results[i] = tokens
    return results

# 3. LDA ê¸°ë°˜ ìµœì  êµ°ì§‘ìˆ˜ íƒìƒ‰ (ì†ë„ ìµœì í™”)
def find_optimal_n_topics_lda_fast(X, texts, dictionary, corpus, vectorizer):
    from sklearn.decomposition import LatentDirichletAllocation
    from sklearn.metrics import silhouette_score

    # 1ë‹¨ê³„: ëŒ€ëµì  ë²”ìœ„ íƒìƒ‰ (50ë‹¨ìœ„)
    n_range_coarse = range(10, 201, 50)
    best_score_coarse = -np.inf
    best_n_coarse = 10

    for n_topics in tqdm(n_range_coarse, desc="1ë‹¨ê³„: ëŒ€ëµì  êµ°ì§‘ìˆ˜ íƒìƒ‰"):
        lda = LatentDirichletAllocation(
            n_components=n_topics,
            learning_method='online',
            batch_size=256,
            max_iter=15,
            random_state=42,
            n_jobs=1
        )
        topic_dist = lda.fit_transform(X)
        silhouette = silhouette_score(X, topic_dist.argmax(axis=1))
        if silhouette > best_score_coarse:
            best_score_coarse = silhouette
            best_n_coarse = n_topics

    # 2ë‹¨ê³„: ìƒì„¸ ë²”ìœ„ íƒìƒ‰ (10ë‹¨ìœ„, ë³‘ë ¬)
    start = max(10, best_n_coarse - 40)
    end = min(200, best_n_coarse + 40)
    n_range_fine = range(start, end+1, 10)

    def train_lda(n):
        lda = LatentDirichletAllocation(
            n_components=n,
            learning_method='online',
            batch_size=256,
            max_iter=15,
            random_state=42,
            n_jobs=1
        )
        lda.fit(X)
        return lda

    print("2ë‹¨ê³„: ìƒì„¸ êµ°ì§‘ìˆ˜ íƒìƒ‰ (ë³‘ë ¬)")
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {executor.submit(train_lda, n): n for n in n_range_fine}
        models = {}
        for future in tqdm(as_completed(futures), total=len(futures)):
            n = futures[future]
            models[n] = future.result()

    # ìµœì  ëª¨ë¸ ì„ ì • (ì‹¤ë£¨ì—£ ì ìˆ˜ë§Œ)
    best_score = -np.inf
    best_n = 10
    for n, model in models.items():
        topic_dist = model.transform(X)
        silhouette = silhouette_score(X, topic_dist.argmax(axis=1))
        if silhouette > best_score:
            best_score = silhouette
            best_n = n

    return best_n

# 4. Gemini ê¸°ë°˜ êµ°ì§‘ìˆ˜ ê²°ì •
def get_optimal_clusters_with_gemini(embeddings, sample_texts, model):
    prompt = f'''[í•œêµ­ì–´ ì§€ì‹œì‚¬í•­]
ìƒ˜í”Œ í…ìŠ¤íŠ¸: {sample_texts[:2000]}
ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ 1ê°œ ì¶”ì²œ (10~200):
{{"optimal_clusters": ì •ìˆ˜}}'''
    for _ in range(3):
        try:
            response = model.generate_content(prompt)
            result = json.loads(response.text.strip())
            return result["optimal_clusters"] if 10 <= result["optimal_clusters"] <= 200 else None
        except Exception:
            time.sleep(1)
    return None

# 5. í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œ ì¶”ì¶œ
def gemini_cluster_keywords(cluster_id, texts, model):
    prompt = f'''[í•œêµ­ì–´ ì§€ì‹œì‚¬í•­]
ë²•ì•ˆ ìƒ˜í”Œ: {texts[:2000]}
4ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ:
{{"keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3", "í‚¤ì›Œë“œ4"]}}'''
    for _ in range(3):
        try:
            response = model.generate_content(prompt)
            return json.loads(response.text.strip())["keywords"][:4]
        except Exception:
            time.sleep(1)
    return ["ê¸°íƒ€"]

if __name__ == '__main__':
    # ì´ˆê¸°í™”
    GEMINI_API_KEY = "ì—¬ê¸°ì—_ì‹¤ì œ_APIí‚¤_ì…ë ¥"
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel('gemini-1.5-pro-latest')

    # ë°ì´í„° ë¡œë“œ
    file_path = Path(r"C:/Users/1-02/Desktop/DAMF2/laws-radar/geovote/data/bill_filtered_final.csv")
    df = pd.read_csv(file_path, encoding='utf-8-sig')

    # 1. ì „ì²˜ë¦¬
    print("ğŸ”„ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¤‘...")
    df['content'] = parallel_gemini_tokenize_and_filter(df['content'].tolist(), model, 4)

    # 2. ì„ë² ë”©
    print("ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")
    embeddings = get_gemini_embeddings(df['content'].tolist())
    if not embeddings:
        print("âŒ ì„ë² ë”© ì‹¤íŒ¨")
        sys.exit(1)

    # 3. êµ°ì§‘ìˆ˜ ê²°ì •
    print("ğŸ” êµ°ì§‘ìˆ˜ ë¶„ì„ ì¤‘...")
    n_clusters = get_optimal_clusters_with_gemini(embeddings, ' '.join(df.sample(100)['content']), model)
    if not n_clusters:
        print("âš ï¸ LDA ë°©ì‹ìœ¼ë¡œ ì „í™˜")
        vectorizer = TfidfVectorizer(max_df=0.7, min_df=5, ngram_range=(1,3), max_features=5000)
        X = vectorizer.fit_transform(df['content'])
        texts = [doc.split() for doc in df['content']]
        n_clusters = find_optimal_n_topics_lda_fast(X, texts, Dictionary(texts), [], vectorizer)
    print(f"âœ… ìµœì¢… êµ°ì§‘ìˆ˜: {n_clusters}")

    # 4. í´ëŸ¬ìŠ¤í„°ë§
    print("ğŸ”„ í´ëŸ¬ìŠ¤í„°ë§ ì¤‘...")
    df['topic'] = KMeans(n_clusters=n_clusters, random_state=42).fit_predict(np.array(embeddings))

    # 5. í‚¤ì›Œë“œ ì¶”ì¶œ
    print("ğŸ”„ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
    cluster_texts = df.groupby('topic')['content'].apply(lambda x: ' '.join(x.sample(min(10, len(x)))))
    with ThreadPoolExecutor(4) as executor:
        futures = {executor.submit(gemini_cluster_keywords, cid, txt, model): cid 
                  for cid, txt in cluster_texts.items()}
        topic_labels = {futures[f]: f.result() for f in tqdm(as_completed(futures), total=len(futures))}
    df['topic_label'] = df['topic'].map(lambda x: ', '.join(topic_labels.get(x, ['ê¸°íƒ€'])))

    # 6. ì €ì¥
    output_path = Path('data/bill_gemini_clustering.csv')
    df[['age', 'title', 'bill_id', 'bill_number', 'content', 'topic', 'topic_label']].to_csv(
        output_path, index=False, encoding='utf-8-sig'
    )
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")
