import sys
from pathlib import Path
import pandas as pd
from kiwipiepy import Kiwi
from tqdm import tqdm
import numpy as np
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# tqdmê³¼ pandas í†µí•©
tqdm.pandas()

# settings.pyë¥¼ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).resolve().parents[1]))
import settings

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
file_path = settings.BASE_DIR / 'geovote' / 'data' / 'bill_filtered_final.csv'
df = pd.read_csv(file_path, encoding='utf-8-sig')

# Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
kiwi = Kiwi(model_type='knlm')

# custom_nouns ë° stopwordsëŠ” ê·¸ëŒ€ë¡œ ë³µì‚¬
custom_nouns = [
    # ì •ë¶€ê¸°ê´€
    'ëŒ€í†µë ¹ë¹„ì„œì‹¤', 'êµ­ê°€ì•ˆë³´ì‹¤', 'ëŒ€í†µë ¹ê²½í˜¸ì²˜', 'í—Œë²•ìƒëŒ€í†µë ¹ìë¬¸ê¸°êµ¬', 'êµ­ê°€ì•ˆì „ë³´ì¥íšŒì˜',
    'ë¯¼ì£¼í‰í™”í†µì¼ìë¬¸íšŒì˜', 'êµ­ë¯¼ê²½ì œìë¬¸íšŒì˜', 'êµ­ê°€ê³¼í•™ê¸°ìˆ ìë¬¸íšŒì˜', 'ê°ì‚¬ì›', 'êµ­ê°€ì •ë³´ì›',
    'ë°©ì†¡í†µì‹ ìœ„ì›íšŒ', 'íŠ¹ë³„ê°ì°°ê´€', 'ê³ ìœ„ê³µì§ìë²”ì£„ìˆ˜ì‚¬ì²˜', 'êµ­ê°€ì¸ê¶Œìœ„ì›íšŒ', 'êµ­ë¬´ì¡°ì •ì‹¤',
    'êµ­ë¬´ì´ë¦¬ë¹„ì„œì‹¤', 'ì¸ì‚¬í˜ì‹ ì²˜', 'ë²•ì œì²˜', 'ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜', 'ê³µì •ê±°ë˜ìœ„ì›íšŒ',
    'êµ­ë¯¼ê¶Œìµìœ„ì›íšŒ', 'ê¸ˆìœµìœ„ì›íšŒ', 'ê°œì¸ì •ë³´ë³´í˜¸ìœ„ì›íšŒ', 'ì›ìë ¥ì•ˆì „ìœ„ì›íšŒ', 'ê¸°íšì¬ì •ë¶€',
    'êµ­ì„¸ì²­', 'ê´€ì„¸ì²­', 'ì¡°ë‹¬ì²­', 'í†µê³„ì²­', 'êµìœ¡ë¶€', 'ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€', 'ìš°ì£¼í•­ê³µì²­',
    'ì™¸êµë¶€', 'ì¬ì™¸ë™í¬ì²­', 'í†µì¼ë¶€', 'ë²•ë¬´ë¶€', 'ê²€ì°°ì²­', 'êµ­ë°©ë¶€', 'ë³‘ë¬´ì²­', 'ë°©ìœ„ì‚¬ì—…ì²­',
    'í–‰ì •ì•ˆì „ë¶€', 'ê²½ì°°ì²­', 'ì†Œë°©ì²­', 'êµ­ê°€ë³´í›ˆë¶€', 'ë¬¸í™”ì²´ìœ¡ê´€ê´‘ë¶€', 'êµ­ê°€ìœ ì‚°ì²­',
    'ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€', 'ë†ì´Œì§„í¥ì²­', 'ì‚°ë¦¼ì²­', 'ì‚°ì—…í†µìƒìì›ë¶€', 'íŠ¹í—ˆì²­', 'ë³´ê±´ë³µì§€ë¶€',
    'ì§ˆë³‘ê´€ë¦¬ì²­', 'í™˜ê²½ë¶€', 'ê¸°ìƒì²­', 'ê³ ìš©ë…¸ë™ë¶€', 'ì—¬ì„±ê°€ì¡±ë¶€', 'êµ­í† êµí†µë¶€',
    'í–‰ì •ì¤‘ì‹¬ë³µí•©ë„ì‹œê±´ì„¤ì²­', 'ìƒˆë§Œê¸ˆê°œë°œì²­', 'í•´ì–‘ìˆ˜ì‚°ë¶€', 'í•´ì–‘ê²½ì°°ì²­', 'ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€',
    # êµ­íšŒ/ì…ë²• ìš©ì–´
    'ìƒì„ìœ„ì›íšŒ', 'ë²•ì œì‚¬ë²•ìœ„ì›íšŒ', 'ì •ë¬´ìœ„ì›íšŒ', 'ê¸°íšì¬ì •ìœ„ì›íšŒ', 'êµìœ¡ìœ„ì›íšŒ',
    'ê³¼í•™ê¸°ìˆ ì •ë³´ë°©ì†¡í†µì‹ ìœ„ì›íšŒ', 'ì™¸êµí†µì¼ìœ„ì›íšŒ', 'êµ­ë°©ìœ„ì›íšŒ', 'í–‰ì •ì•ˆì „ìœ„ì›íšŒ',
    'ë¬¸í™”ì²´ìœ¡ê´€ê´‘ìœ„ì›íšŒ', 'ë†ë¦¼ì¶•ì‚°ì‹í’ˆí•´ì–‘ìˆ˜ì‚°ìœ„ì›íšŒ', 'ì‚°ì—…í†µìƒìì›ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ìœ„ì›íšŒ',
    'ë³´ê±´ë³µì§€ìœ„ì›íšŒ', 'í™˜ê²½ë…¸ë™ìœ„ì›íšŒ', 'êµ­í† êµí†µìœ„ì›íšŒ', 'ì •ë³´ìœ„ì›íšŒ', 'ì—¬ì„±ê°€ì¡±ìœ„ì›íšŒ',
    'ì˜ˆì‚°ê²°ì‚°íŠ¹ë³„ìœ„ì›íšŒ', 'íŠ¹ë³„ìœ„ì›íšŒ', 'ì†Œìœ„ì›íšŒ', 'ë²•ì•ˆì‹¬ì‚¬ì†Œìœ„', 'ì˜ì•ˆ', 'ë²•ë¥ ì•ˆ',
    'ì˜ˆì‚°ì•ˆ', 'ë™ì˜ì•ˆ', 'ìŠ¹ì¸ì•ˆ', 'ê²°ì˜ì•ˆ', 'ê±´ì˜ì•ˆ', 'ê·œì¹™ì•ˆ', 'ì„ ì¶œì•ˆ', 'ë°œì˜', 'ì œì¶œ',
    'ì œì•ˆ', 'ì œì˜', 'ì˜ê²°', 'ë¶€ê²°', 'íê¸°', 'ê°€ê²°', 'ì±„íƒ', 'ì…ë²•ì˜ˆê³ ', 'ê³µí¬', 'ì‹œí–‰',
    'ê°œì •', 'ì œì •', 'íì§€', 'ì¼ë¶€ê°œì •', 'ì „ë¶€ê°œì •',
    # ë²•ë¥  ë¶„ì•¼
    'í—Œë²•', 'ë¯¼ë²•', 'í˜•ë²•', 'ìƒë²•', 'í–‰ì •ë²•', 'ë…¸ë™ë²•', 'ì„¸ë²•', 'í™˜ê²½ë²•', 'ì •ë³´í†µì‹ ë²•',
    'ê¸ˆìœµë²•', 'ë³´ê±´ì˜ë£Œë²•', 'êµìœ¡ë²•', 'ë¬¸í™”ì˜ˆìˆ ë²•', 'ë†ë¦¼ë²•', 'ê±´ì„¤ë²•', 'í•´ì–‘ë²•', 'ê¹€ì˜ë€ë²•',
    'ë¶€ì •ì²­íƒê¸ˆì§€ë²•', 'ê³µì§ììœ¤ë¦¬ë²•', 'ì •ì¹˜ìê¸ˆë²•', 'ê³µì§ì„ ê±°ë²•', 'ì „ê¸°í†µì‹ ì‚¬ì—…ë²•',
    'ê°œì¸ì •ë³´ë³´í˜¸ë²•', 'êµ­ê°€ìœ ì‚°ìˆ˜ë¦¬ê¸°ìˆ ìœ„ì›íšŒ', 'ë¶€ê°€ê°€ì¹˜ì„¸ë²•', 'ìˆ˜ì…ì‹í’ˆì•ˆì „ê´€ë¦¬íŠ¹ë³„ë²•',
    'ë‹¤ë¬¸í™”ê°€ì¡±ì§€ì›ë²•',
    # ë””ì§€í„¸/ê¸°ìˆ 
    'ì¸ê³µì§€ëŠ¥', 'ë¹…ë°ì´í„°', 'ì‚¬ë¬¼ì¸í„°ë„·', 'í´ë¼ìš°ë“œ', 'ë¸”ë¡ì²´ì¸', 'ë©”íƒ€ë²„ìŠ¤', 'ë””ì§€í„¸í”Œë«í¼',
    'ì „ìì •ë¶€', 'ë””ì§€í„¸ì „í™˜', 'ì‚¬ì´ë²„ë³´ì•ˆ', 'ë””ì§€í„¸ë‰´ë”œ', 'ìŠ¤ë§ˆíŠ¸ì‹œí‹°', 'ë””ì§€í„¸í¬ìš©',
    'ì˜¨ë¼ì¸í”Œë«í¼', 'ì „ììƒê±°ë˜',
    # ì‚¬íšŒ í˜„ì•ˆ
    'ì½”ë¡œë‚˜19', 'ê°ì—¼ë³‘', 'ë°±ì‹ ', 'ë°©ì—­', 'ì‚¬íšŒì ê±°ë¦¬ë‘ê¸°', 'ì¬ë‚œì§€ì›ê¸ˆ', 'ê¸°í›„ë³€í™”',
    'ë¯¸ì„¸ë¨¼ì§€', 'íê¸°ë¬¼ì²˜ë¦¬', 'ì¬í™œìš©', 'ìˆœí™˜ê²½ì œ', 'ì  ë”í‰ë“±', 'ì„±í¬ë¡±', 'ì„±í­ë ¥', 'ìŠ¤í† í‚¹',
    'ê°€ì •í­ë ¥', 'ë””ì§€í„¸ì„±ë²”ì£„', 'ì²­ë…„ì •ì±…', 'ì²­ë…„ê³ ìš©', 'ì²­ë…„ì£¼íƒ', 'í•™ìê¸ˆëŒ€ì¶œ', 'êµìœ¡ê²©ì°¨'
]
for noun in custom_nouns:
    kiwi.add_user_word(noun, 'NNG', 9.0)

stopwords = {
    'ì¡°', 'í•­', 'í˜¸', 'ê²½ìš°', 'ë“±', 'ìˆ˜', 'ê²ƒ', 'ì´', 'ì°¨', 'í›„', 'ì´ìƒ', 'ì´í•˜', 'ì´ë‚´',
    'ì•ˆ', 'ì†Œ', 'ëŒ€', 'ì ', 'ê°„', 'ê³³', 'í•´ë‹¹', 'ì°¨', 'ì™¸', 'ê²½ìš°', 'ë‚˜', 'ë°”', 'ì‹œ',
    'ê´€ë ¨', 'ê´€í•˜ì—¬', 'ëŒ€í•˜ì—¬', 'ë”°ë¼', 'ë”°ë¥¸', 'ìœ„í•˜ì—¬', 'ì˜í•˜ì—¬', 'ë•Œ', 'ê°', 'ì', 'ì¸',
    'ë‚´', 'ì¤‘', 'ë•Œë¬¸', 'ìœ„í•´', 'í†µí•´', 'ë¶€í„°', 'ê¹Œì§€', 'ë™ì•ˆ', 'ì‚¬ì´', 'ê¸°ì¤€', 'ë³„ë„',
    'ë³„ì²¨', 'ë³„í‘œ', 'ì œí•œ', 'íŠ¹ì¹™', 'ê°€ëŠ¥', 'ê³¼ì •', 'ê¸°ë°˜', 'ê¸°ì¡´', 'ê·¼ê±°', 'ê¸°ëŠ¥', 'ë°©ì‹',
    'ë²”ìœ„', 'ì‚¬í•­', 'ì‹œì ', 'ì˜í•œ', 'ì¸í•œ', 'ìµœê·¼', '?', 'ë…„', 'ì¥', 'í•´', 'ëª…', 'ë‚ ', 'íšŒ',
    'ë™', 'ë°', 'êµ­', 'ë°–', 'ì†', 'ì‹', 'ìœµ', 'ë°–', 'ê·œ', 'í˜„í–‰ë²•','ì§', 'ë²”', 'ë§Œ', 'ì…', 'ì§',
    'ì‹ ',
    # ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ì¶”ê°€ ë¶ˆìš©ì–´ (ì•½ 200ê°œ)
    'ê°€', 'ê°€ë ¹', 'ê°€ì§€', 'ê°ê°', 'ê°ì', 'ê°ì¢…', 'ê°–ê³ ë§í•˜ìë©´', 'ê°™ë‹¤', 'ê°™ì´', 'ê±°ë‹ˆì™€',
    'ê±°ì˜', 'ê²ƒê³¼ ê°™ì´', 'ê²ƒë“¤', 'ê²Œë‹¤ê°€', 'ê²¨ìš°', 'ê²°ê³¼ì— ì´ë¥´ë‹¤', 'ê²°êµ­', 'ê²°ë¡ ì„ ë‚¼ ìˆ˜ ìˆë‹¤',
    'ê²¸ì‚¬ê²¸ì‚¬', 'ê³ ë ¤í•˜ë©´', 'ê³ ë¡œ', 'ê³§', 'ê³¼', 'ê³¼ì—°', 'ê´€ê³„ê°€ ìˆë‹¤', 'ê´€ê³„ì—†ì´', 'ê´€ë ¨ì´ ìˆë‹¤',
    'ê´€í•œ', 'ê´€í•´ì„œëŠ”', 'êµ¬ì²´ì ìœ¼ë¡œ', 'ê·¸', 'ê·¸ë“¤', 'ê·¸ë•Œ', 'ê·¸ë˜', 'ê·¸ë˜ë„', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ¬ë‚˜',
    'ê·¸ëŸ¬ë‹ˆ', 'ê·¸ëŸ¬ë‹ˆê¹Œ', 'ê·¸ëŸ¬ë©´', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ê·¸ëŸ° ê¹Œë‹­ì—', 'ê·¸ëŸ°ë°', 'ê·¸ëŸ¼', 'ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ ',
    'ê·¸ë ‡ê²Œ í•¨ìœ¼ë¡œì¨', 'ê·¸ë ‡ì§€', 'ê·¸ë ‡ì§€ ì•Šë‹¤ë©´', 'ê·¸ë ‡ì§€ ì•Šìœ¼ë©´', 'ê·¸ë ‡ì§€ë§Œ', 'ê·¸ë¦¬ê³ ', 'ê·¸ë¦¬í•˜ì—¬',
    'ê·¸ë§Œì´ë‹¤', 'ê·¸ì— ë”°ë¥´ëŠ”', 'ê·¸ì €', 'ê·¼ê±°ë¡œ', 'ê¸°íƒ€', 'ê¹Œì§€', 'ê¹Œì§€ë„', 'ë‚˜', 'ë‚˜ë¨¸ì§€ëŠ”', 'ë‚¨ë“¤',
    'ë„ˆ', 'ë„ˆí¬', 'ë„¤', 'ë…¼í•˜ì§€ ì•Šë‹¤', 'ë‹¤ë¥¸', 'ë‹¤ë§Œ', 'ë‹¤ì†Œ', 'ë‹¤ìˆ˜', 'ë‹¤ì‹œ ë§í•˜ìë©´', 'ë‹¤ìŒ',
    'ë‹¤ìŒì—', 'ë‹¤ìŒìœ¼ë¡œ', 'ë‹¨ì§€', 'ë‹¹ì‹ ', 'ëŒ€í•˜ë©´', 'ëŒ€í•´ ë§í•˜ìë©´', 'ëŒ€í•´ì„œ', 'ë”êµ¬ë‚˜', 'ë”êµ°ë‹¤ë‚˜',
    'ë”ë¼ë„', 'ë”ë¶ˆì–´', 'ë™ì‹œì—', 'ëœë°”ì—ì•¼', 'ëœì´ìƒ', 'ë‘ë²ˆì§¸ë¡œ', 'ë‘˜', 'ë“±ë“±', 'ë”°ë¼', 'ë”°ìœ„',
    'ë”°ì§€ì§€ ì•Šë‹¤', 'ë•Œê°€ ë˜ì–´', 'ë˜', 'ë˜í•œ', 'ë¼ í•´ë„', 'ë ¹', 'ë¡œ', 'ë¡œ ì¸í•˜ì—¬', 'ë¡œë¶€í„°', 'ë¥¼',
    'ë§ˆìŒëŒ€ë¡œ', 'ë§ˆì €', 'ë§‰ë¡ í•˜ê³ ', 'ë§Œ ëª»í•˜ë‹¤', 'ë§Œì•½', 'ë§Œì•½ì—', 'ë§Œì€ ì•„ë‹ˆë‹¤', 'ë§Œì¼', 'ë§Œí¼',
    'ë§í•˜ìë©´', 'ë§¤', 'ëª¨ë‘', 'ë¬´ë µ', 'ë¬´ìŠ¨', 'ë¬¼ë¡ ', 'ë°', 'ë°”ê¾¸ì–´ë§í•˜ë©´', 'ë°”ê¾¸ì–´ì„œ ë§í•˜ë©´',
    'ë°”ë¡œ', 'ë°–ì— ì•ˆëœë‹¤', 'ë°˜ëŒ€ë¡œ', 'ë°˜ë“œì‹œ', 'ë²„ê¸ˆ', 'ë³´ëŠ”ë°ì„œ', 'ë³¸ëŒ€ë¡œ', 'ë¶€ë¥˜ì˜ ì‚¬ëŒë“¤',
    'ë¶ˆêµ¬í•˜ê³ ', 'ì‚¬', 'ì‚¼', 'ìƒëŒ€ì ìœ¼ë¡œ ë§í•˜ìë©´', 'ì„¤ë ¹', 'ì„¤ë§ˆ', 'ì…‹', 'ì†Œìƒ', 'ìˆ˜', 'ìŠµë‹ˆê¹Œ',
    'ìŠµë‹ˆë‹¤', 'ì‹œê°„', 'ì‹œì‘í•˜ì—¬', 'ì‹¤ë¡œ', 'ì‹¬ì§€ì–´', 'ì•„', 'ì•„ë‹ˆ', 'ì•„ë‹ˆë¼ë©´', 'ì•„ë‹ˆë©´', 'ì•„ì•¼',
    'ì•„ìš¸ëŸ¬', 'ì•ˆ ê·¸ëŸ¬ë©´', 'ì•Šê¸° ìœ„í•˜ì—¬', 'ì•Œ ìˆ˜ ìˆë‹¤', 'ì•ì—ì„œ', 'ì•½ê°„', 'ì–‘ì', 'ì–´', 'ì–´ëŠ',
    'ì–´ë””', 'ì–´ë•Œ', 'ì–´ë– í•œ', 'ì–´ë–¤', 'ì–´ë–»ê²Œ', 'ì–´ì§¸ì„œ', 'ì–´ì¨‹ë“ ', 'ì–¸ì œ', 'ì–¼ë§ˆ', 'ì–¼ë§ˆë§Œí¼',
    'ì—‰ì—‰', 'ì—', 'ì—ê²Œ', 'ì—ì„œ', 'ì—ì´', 'ì—”', 'ì˜', 'ì˜ˆ', 'ì˜¬', 'ì™€', 'ì™€ë¥´ë¥´', 'ì™€ì•„', 'ì™œ',
    'ì™¸ì—', 'ìš”', 'ìš°ë¦¬', 'ì›', 'ì›”', 'ìœ„ì—ì„œ ì„œìˆ í•œë°”ì™€ê°™ì´', 'ìœ™ìœ™', 'ìœ¡', 'ìœ¼ë¡œ', 'ìœ¼ë¡œì„œ',
    'ìœ¼ë¡œì¨', 'ì„', 'ì‘', 'ì‘ë‹¹', 'ì˜', 'ì˜ê±°í•˜ì—¬', 'ì˜ì§€í•˜ì—¬', 'ì˜í•´', 'ì´', 'ì´ë¥´ë‹¤', 'ì´ìª½',
    'ì¸ì  ', 'ì¼', 'ì¼ê²ƒì´ë‹¤', 'ì„ì— í‹€ë¦¼ì—†ë‹¤', 'ìê¸°', 'ìê¸°ì§‘', 'ìë§ˆì', 'ìì‹ ', 'ì ê¹', 'ì ì‹œ',
    'ì €', 'ì €ê¸°', 'ì €ìª½', 'ì „ë¶€', 'ì „ì', 'ì •ë„ì— ì´ë¥´ë‹¤', 'ì œ', 'ì œì™¸í•˜ê³ ', 'ì¡°ì°¨', 'ì¢‹ì•„', 'ì¢ì¢',
    'ì£¼', 'ì£¼ì €í•˜ì§€ ì•Šê³ ', 'ì¤„', 'ì¤‘ì´ë‹¤', 'ì¦ˆìŒ', 'ì¦‰', 'ì§€ê¸ˆ', 'ì§€ë§ê³ ', 'ì§„ì§œë¡œ', 'ìª½ìœ¼ë¡œ',
    'ì¯¤', 'ì°¨ë¼ë¦¬', 'ì°¸', 'ì²«ë²ˆì§¸ë¡œ', 'ì³‡', 'ì½¸ì½¸', 'ì¾…ì¾…', 'ì¿µ', 'í¼', 'íƒ€ë‹¤', 'í†µí•˜ë‹¤', 'í‹ˆíƒ€',
    'íŒ', 'í½', 'í•˜', 'í•˜ê²Œë ê²ƒì´ë‹¤', 'í•˜ê²Œí•˜ë‹¤', 'í•˜ê² ëŠ”ê°€', 'í•˜ê³ ', 'í•˜ê³ ìˆì—ˆë‹¤', 'í•˜ê³¤í•˜ì˜€ë‹¤',
    'í•˜êµ¬ë‚˜', 'í•˜ê¸° ë•Œë¬¸ì—', 'í•˜ê¸°ë§Œ í•˜ë©´', 'í•˜ê¸°ë³´ë‹¤ëŠ”', 'í•˜ê¸°ì—', 'í•˜ë‚˜', 'í•˜ëŠë‹ˆ', 'í•˜ëŠ” ê²ƒë§Œ',
    'í•˜ëŠ” í¸ì´ ë‚«ë‹¤', 'í•˜ëŠ”ê²ƒë„', 'í•˜ë”ë¼ë„', 'í•˜ë„ë‹¤', 'í•˜ë„ë¡ì‹œí‚¤ë‹¤', 'í•˜ë„ë¡í•˜ë‹¤', 'í•˜ë“ ì§€',
    'í•˜ë ¤ê³ í•˜ë‹¤', 'í•˜ë§ˆí„°ë©´', 'í•˜ë©´ í• ìˆ˜ë¡', 'í•˜ë©´ì„œ', 'í•˜ë¬¼ë©°', 'í•˜ì—¬ê¸ˆ', 'í•˜ì—¬ì•¼', 'í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´',
    'í•˜ì§€ ì•Šë„ë¡', 'í•˜ì§€ë§ˆ', 'í•˜ì§€ë§ˆë¼', 'í•˜ì²œ', 'í•˜í’ˆ', 'í•œ', 'í•œ ì´ìœ ëŠ”', 'í•œ í›„', 'í•œë‹¤ë©´',
    'í•œë‹¤ë©´ ëª°ë¼ë„', 'í•œë°', 'í•œë§ˆë””', 'í•œì ì´ìˆë‹¤', 'í•œì¼ ìœ¼ë¡œëŠ”', 'í•œí¸', 'í•  ë”°ë¦„ì´ë‹¤', 'í•  ìƒê°ì´ë‹¤',
    'í•  ì¤„ ì•ˆë‹¤', 'í•  ì§€ê²½ì´ë‹¤', 'í• ë•Œ', 'í• ë§Œí•˜ë‹¤', 'í• ë§ì •', 'í• ë¿', 'í•¨ê»˜', 'í•´ë„',
    'í•´ë´ìš”', 'í•´ì„œëŠ”', 'í•´ì•¼í•œë‹¤', 'í•´ìš”', 'í–ˆì–´ìš”', 'í˜•ì‹ìœ¼ë¡œ ì“°ì—¬', 'í˜¹ì‹œ', 'í˜¹ì€', 'í˜¼ì', 'í›¨ì”¬',
    'íœ˜ìµ', 'íœ´', 'íí', 'í˜ì…ì–´',
    # ëª©ì°¨ ê¸°í˜¸
    'ê°€.', 'ë‚˜.', 'ë‹¤.', 'ë¼.', 'ë§ˆ.', 'ë°”.', 'ì‚¬.', 'ì•„.', 'ì.', 'ì°¨.', 'ì¹´.', 'íƒ€.', 'íŒŒ.', 'í•˜.',
    '1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.', '10.', '11.', '12.', '13.', '14.', '15.',
    'â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤', 'â‘¥', 'â‘¦', 'â‘§', 'â‘¨', 'â‘©',
    'â… ', 'â…¡', 'â…¢', 'â…£', 'â…¤', 'â…¥', 'â…¦', 'â…§', 'â…¨', 'â…©',
    'i.', 'ii.', 'iii.', 'iv.', 'v.', 'vi.', 'vii.', 'viii.', 'ix.', 'x.',
    # ê¸°íƒ€... '.', '-', 'Â·', 'â€¢', 'â€»', '(', ')', '[', ']', '{', '}', '/', '\\',
    # ë²•ë¥  ì¼ë°˜ ìš©ì–´
    'ë²•ë¥ ', 'ë²•', 'ì¡°ë¡€', 'ê·œì •', 'ì¡°í•­', 'ì¡°ë¬¸', 'ì¡°ì¹˜', 'ì¡°ì •', 'ê·œì¹™',
    'ë²•ì•ˆ', 'ì…ë²•', 'ê°œì •', 'ì œì •', 'ì‹œí–‰', 'ê³µí¬', 'íì§€', 'ì¼ë¶€ê°œì •',
    'ì „ë¶€ê°œì •', 'ë™ì˜ì•ˆ', 'ìŠ¹ì¸ì•ˆ', 'ê²°ì˜ì•ˆ', 'ê±´ì˜ì•ˆ', 'ê·œì¹™ì•ˆ', 'ì„ ì¶œì•ˆ',
    'ë°œì˜', 'ì œì¶œ', 'ì œì•ˆ', 'ì œì˜', 'ì˜ê²°', 'ë¶€ê²°', 'íê¸°', 'ê°€ê²°', 'ì±„íƒ',
    'ì…ë²•ì˜ˆê³ ', 'ì²œë§Œ', 'ê¸°ê´€', 'ê¸°ê°„',
    # ë²•ë¥  íŠ¹í™” ì¶”ê°€ ë¶ˆìš©ì–´
    'ë²•ë¥ ', 'ë²•ì•ˆ', 'ì…ë²•', 'ê°œì •', 'ì œì •', 'ì‹œí–‰', 'ê³µí¬', 'íì§€', 'ì¡°ë¡€', 'ê·œì •',
    'ì¡°í•­', 'ì¡°ë¬¸', 'ì¡°ì¹˜', 'ì¡°ì •', 'ê·œì¹™', 'ë°œì˜', 'ì œì¶œ', 'ì œì•ˆ', 'ì˜ê²°', 'ë¶€ê²°',
    'ê°€ê²°', 'ì±„íƒ', 'ì‹¬ì˜', 'ì²˜ë¦¬', 'ì•ˆê±´', 'ì˜ì•ˆ', 'ìƒì •', 'ì˜ê²°', 'ì¬ì ', 'ìœ„ì›',
    'ìœ„ì›íšŒ', 'ì†Œìœ„ì›íšŒ', 'ë³¸íšŒì˜', 'ìƒì„ìœ„', 'íŠ¹ë³„ìœ„', 'ë²•ì œì²˜', 'ì…ë²•ì˜ˆê³ ',
}

preserve_terms = {
    'ë²•ë¥ ', 'ë²•ì•ˆ', 'ì…ë²•', 'ê°œì •', 'ì œì •', 'ì‹œí–‰', 'ê³µí¬', 'íì§€',
    'ì¡°ë¡€', 'ê·œì •', 'ì¡°í•­', 'ì¡°ë¬¸', 'ì˜ê²°', 'ë¶€ê²°', 'ê°€ê²°', 'ì±„íƒ'
}
stopwords = {term for term in stopwords if term not in preserve_terms}

# ğŸš© ë°˜ë“œì‹œ ì œì™¸í•  í‚¤ì›Œë“œ ì¶”ê°€
excluded_terms = {
    'ì£¼ìš”', 'ìˆ˜ì‚¬', 'ê´€ë ¨', 'ì‚¬í•­', 'ì •ì±…', 'ëŒ€ìƒ', 'ë°©ì•ˆ', 'ì¶”ì§„', 'ê°•í™”', 'í™œì„±í™”',
    'ê°œì„ ', 'ì§€ì›', 'í™•ëŒ€', 'ì¡°ì¹˜', 'í•„ìš”', 'í˜„í™©', 'ê¸°ë°˜', 'ê³¼ì •', 'ê¸°ì¡´', 'ê·¼ê±°',
    'ê¸°ëŠ¥', 'ë°©ì‹', 'ë²”ìœ„', 'ì‹œì ', 'ìµœê·¼', 'ì‚¬ì—…', 'ê³„íš', 'ê³ ë ¤', 'ë¬¸ì œ', 'ë¶€ë¶„',
    "í”¼ì„±ë…„í›„ê²¬ì¸", "í”¼í•œì •í›„ê²¬ì¸", "ê³ ìœ„ê³µì§ìë²”ì£„ìˆ˜ì‚¬ì²˜", "ë¬¸í™”ì²´ìœ¡ê´€ê´‘ë¶€ ì¥ê´€",
    "êµ­ë¯¼ê¶Œìµìœ„ì›íšŒ ê¶Œê³ ì•ˆ", "ë²Œê¸ˆì•¡",
    "ì‚°ì—…í†µìƒìì›ë¶€", "ì‚°ì—…í†µìƒìì›ë¶€ ì¥ê´€", "ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€", "ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€ ì¥ê´€"
}

def preprocess_text(text):
    text = str(text)
    text = text.replace('ï¼Ÿ', '').replace('?', '')
    text = re.sub(r'[^\uAC00-\uD7A3a-zA-Z0-9\s]', ' ', text)
    patterns = [
        (r'(ìˆ˜ì…)\s+(ì—…)', r'\1ì—…'),
        (r'(ë°©ì œ)\s+(ì—…)', r'\1ì—…'),
        (r'(ì œì¡°)\s+(ì—…)', r'\1ì—…'),
        (r'(íŒë§¤)\s+(ì—…)', r'\1ì—…'),
        (r'(ì„œë¹„ìŠ¤)\s+(ì—…)', r'\1ì—…'),
        (r'(ìœ í†µ)\s+(ì—…)', r'\1ì—…')
    ]
    for pattern, replacement in patterns:
        text = re.sub(pattern, replacement, text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def extract_nouns(text):
    try:
        text = preprocess_text(text)
        tokens = kiwi.tokenize(text)
        words = []
        for token in tokens:
            if not token.tag.startswith('N'):
                continue
            if any(c.isdigit() for c in token.form):
                continue
            if token.form in excluded_terms:
                continue
            if token.form in stopwords:
                continue
            words.append(token.form)
        return ' '.join(words)
    except Exception as e:
        print(f"Error processing text: {e}")
        return ''

def remove_single_char_words(text):
    if pd.isnull(text):
        return ""
    words = text.split()
    filtered_words = [word for word in words if len(word) > 1]
    return ' '.join(filtered_words)

# ë°ì´í„° ì „ì²˜ë¦¬ (ì§„í–‰ë¥  í‘œì‹œ)
df['content'] = df['content'].progress_apply(extract_nouns)
df['content'] = df['content'].apply(remove_single_char_words)

# LDAìš© ë²¡í„°í™” (CountVectorizer ì‚¬ìš©)
vectorizer = CountVectorizer(
    max_df=0.5,
    min_df=10,
    ngram_range=(1, 2),
    max_features=5000,
    token_pattern=r"(?u)\b\w+\b"
)
X = vectorizer.fit_transform(df['content'])

# LDA í† í”½ ëª¨ë¸ ì ìš©
n_topics = 200  # í† í”½ ìˆ˜ëŠ” ë°ì´í„°ì— ë”°ë¼ ì¡°ì •
lda = LatentDirichletAllocation(
    n_components=n_topics,
    max_iter=20,
    learning_method='batch',
    random_state=42,
    evaluate_every=1,
    n_jobs=-1
)
lda.fit(X)
df['topic'] = lda.transform(X).argmax(axis=1)

# í† í”½ë³„ ëŒ€í‘œ í‚¤ì›Œë“œ ì¶”ì¶œ
def get_top_words(model, feature_names, n_top_words=4):
    topic_keywords = {}
    for topic_idx, topic in enumerate(model.components_):
        top_indices = topic.argsort()[::-1][:n_top_words*2]  # ë„‰ë„‰í•˜ê²Œ í›„ë³´ ì¶”ì¶œ
        keywords = []
        for i in top_indices:
            word = feature_names[i]
            # ë¶ˆìš©ì–´, ì œì™¸ì–´, í•œ ê¸€ì, ì¤‘ë³µ ë‹¨ì–´ ëª¨ë‘ ì œê±°
            if word in stopwords or word in excluded_terms or len(word) == 1:
                continue
            if not any(word in k for k in keywords):  # ë¶€ë¶„ ì¤‘ë³µ ì œê±°
                keywords.append(word)
            if len(keywords) >= n_top_words:
                break
        topic_keywords[topic_idx] = ', '.join(keywords)
    return topic_keywords

feature_names = np.array(vectorizer.get_feature_names_out())
topic_keywords = get_top_words(lda, feature_names, n_top_words=7)
df['topic_label'] = df['topic'].apply(lambda x: f"{x} ({topic_keywords[x]})")

# ê²°ê³¼ ì €ì¥
output_path = settings.BASE_DIR / 'keword_clustering' / 'data' / 'bill_keyword_clustering2.csv'
df.to_csv(output_path, index=False, encoding='utf-8-sig')

# í† í”½ë³„ ëŒ€í‘œ í‚¤ì›Œë“œ ë° ë¬¸ì„œ ìˆ˜ ì¶œë ¥
print("\n[ìµœì¢… í† í”½(ì£¼ì œ) í˜„í™©]")
for topic_num in sorted(topic_keywords.keys()):
    count = (df['topic'] == topic_num).sum()
    print(f"í† í”½ {topic_num} ({count}ê°œ ë²•ì•ˆ)")
    print(f"  ëŒ€í‘œ í‚¤ì›Œë“œ: {topic_keywords[topic_num]}\n")
print("âœ… í† í”½ ëª¨ë¸ë§(LDA) ì™„ë£Œ ë° ê²°ê³¼ ì €ì¥")
