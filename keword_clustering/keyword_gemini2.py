# -*- coding: utf-8 -*-
import sys
from pathlib import Path
import pandas as pd
from kiwipiepy import Kiwi
from tqdm import tqdm
import numpy as np
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.metrics import silhouette_score
from gensim.models import CoherenceModel
from gensim.corpora import Dictionary
import google.generativeai as genai
import time
import json
import csv
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache

tqdm.pandas()

# ì½”ë“œë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ í•¨ìˆ˜
def extract_json_from_codeblock(text):
    text = text.strip()
    text = re.sub(r"^```(json)?\s*", "", text, flags=re.MULTILINE)
    text = re.sub(r"\s*```$", "", text, flags=re.MULTILINE)
    return text.strip()

# ë¶ˆìš©ì–´/ì œì™¸ë‹¨ì–´ ì •ì œ í•¨ìˆ˜ (frozenset ëŒ€ì‘)
@lru_cache(maxsize=1000)
def refine_stopwords_excluded_terms(text_sample, model, initial_stopwords, initial_excluded_terms, preserve_terms):
    initial_stopwords = set(initial_stopwords)
    initial_excluded_terms = set(initial_excluded_terms)
    preserve_terms = set(preserve_terms)
    
    text_sample = re.sub(r'[{}]', '', text_sample)
    text_sample = text_sample.replace('"', '').replace("'", '').replace('\n', ' ').replace('\r', ' ')
    text_sample = re.sub(r'\s+', ' ', text_sample).strip()
    
    prompt = (
        '[í•œêµ­ì–´ ì§€ì‹œì‚¬í•­]\n'
        'ë‹¤ìŒ í…ìŠ¤íŠ¸ ìƒ˜í”Œì„ ë°”íƒ•ìœ¼ë¡œ ë²•ì•ˆ ë¬¸ì„œì—ì„œ ì œê±°í•´ì•¼ í•  ë¶ˆìš©ì–´ì™€ ì œì™¸ ë‹¨ì–´ë¥¼ ê°ê° ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„±:\n'
        '- ë¶ˆìš©ì–´: ë²•ì•ˆì˜ ì£¼ì œì™€ ë¬´ê´€í•œ ì¼ë°˜ì  ë‹¨ì–´ (ì˜ˆ: \'ê²ƒ\', \'ìˆ˜\')\n'
        '- ì œì™¸ ë‹¨ì–´: ì£¼ì œì™€ ê´€ë ¨ ìˆì§€ë§Œ ë„ˆë¬´ ì¼ë°˜ì ì´ê±°ë‚˜ ëª¨í˜¸í•œ ë‹¨ì–´ (ì˜ˆ: \'ì§€ì›\', \'ê°•í™”\')\n'
        '- ê° ë¦¬ìŠ¤íŠ¸ëŠ” ìµœëŒ€ 100ê°œ ë‹¨ì–´ë¡œ ì œí•œ\n'
        '- ë²•ë¥  ìš©ì–´ (ì˜ˆ: \'ë²•ë¥ \', \'ì¡°í•­\')ëŠ” ì œì™¸í•˜ì§€ ì•ŠìŒ\n'
        '- ê²°ê³¼ëŠ” JSON í˜•ì‹ìœ¼ë¡œ, {"stopwords": [], "excluded_terms": []} ë°˜í™˜\n'
        '- ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ë©”íƒ€ ì •ë³´, ì½”ë“œë¸”ë¡, ê¸°íƒ€ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨ì‹œí‚¤ì§€ ì•ŠìŒ\n'
        '- JSON ì´ì™¸ì˜ ë¬¸ìëŠ” 1byteë„ í—ˆìš©ë˜ì§€ ì•ŠìŒ\n\n'
        f'í…ìŠ¤íŠ¸ ìƒ˜í”Œ: {text_sample[:2000]}'
    )
    
    max_retries = 5
    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt)
            if response and response.text:
                cleaned_response = extract_json_from_codeblock(response.text)
                print(f"ğŸ” Gemini ì‘ë‹µ (ì‹œë„ {attempt+1}): {cleaned_response[:200]}...")

                if not cleaned_response.strip():
                    print("âš ï¸ Gemini API ì‘ë‹µì´ ë¹„ì—ˆìŠµë‹ˆë‹¤.")
                    return list(initial_stopwords), list(initial_excluded_terms)

                if cleaned_response.count('{') != cleaned_response.count('}'):
                    print("âš ï¸ JSON êµ¬ì¡° ë¶ˆì™„ì „(ê´„í˜¸ ë¶ˆì¼ì¹˜), ì‘ë‹µ ë¬´ì‹œ")
                    return list(initial_stopwords), list(initial_excluded_terms)

                try:
                    result = json.loads(cleaned_response)
                    stopwords = [w for w in result.get('stopwords', []) if w not in preserve_terms and len(w) > 1]
                    excluded = [w for w in result.get('excluded_terms', []) if w not in preserve_terms and len(w) > 1]
                    return stopwords, excluded
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                    print(f"ğŸ“œ ì‘ë‹µ ë‚´ìš©: {cleaned_response[:500]}")
                    if ',' in cleaned_response:
                        kw_list = [kw.strip() for kw in cleaned_response.split(',') if kw.strip()]
                        stopwords = kw_list[:50]
                        excluded = kw_list[50:100]
                        return stopwords, excluded
            time.sleep(2 ** attempt)
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜ (ì‹œë„ {attempt+1}/{max_retries}): {str(e)[:100]}")
            time.sleep(2 ** attempt)
    return list(initial_stopwords), list(initial_excluded_terms)

# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_text(text):
    text = str(text).replace('ï¼Ÿ', '').replace('?', '')
    text = re.sub(r'[^\uAC00-\uD7A3a-zA-Z0-9\s]', ' ', text)
    patterns = [(r'(\w+)\s+(ì—…)', r'\1ì—…')]
    for pattern, replacement in patterns:
        text = re.sub(pattern, replacement, text)
    return re.sub(r'\s+', ' ', text).strip()

# í† í° ì¶”ì¶œ í•¨ìˆ˜
def extract_tokens(texts, kiwi, excluded_terms, stopwords):
    results = []
    for text in texts:
        try:
            tokens = kiwi.tokenize(preprocess_text(text))
            words = [
                token.form for token in tokens
                if (token.tag.startswith(('N', 'V', 'VA')) and
                    not any(c.isdigit() for c in token.form) and
                    token.form not in excluded_terms and
                    token.form not in stopwords and
                    len(token.form) > 1)
            ]
            results.append(' '.join(words))
        except Exception as e:
            print(f"ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            results.append('')
    return results

# ìµœì  í† í”½ ìˆ˜ ê³„ì‚° í•¨ìˆ˜
def find_optimal_n_topics(X, texts, dictionary, corpus, vectorizer, min_topics=10, max_topics=200, step=10):
    best_n_topics = min_topics
    best_silhouette = -1
    best_coherence = 0
    print("\nğŸ” ì‹¤ë£¨ì—£ ì ìˆ˜ë¡œ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê³„ì‚° ì¤‘...")
    for n_topics in tqdm(range(min_topics, max_topics + 1, step)):
        try:
            lda = LatentDirichletAllocation(
                n_components=n_topics,
                max_iter=20,
                learning_method='batch',
                random_state=42,
                n_jobs=1
            )
            topic_dist = lda.fit_transform(X)
            silhouette = silhouette_score(X, topic_dist.argmax(axis=1))
            
            topics = []
            for topic in lda.components_:
                top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]
                topics.append(top_words)
            
            coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')
            coherence = coherence_model.get_coherence()
            
            print(f"n_topics={n_topics}, ì‹¤ë£¨ì—£: {silhouette:.4f}, Coherence: {coherence:.4f}")
            if silhouette > best_silhouette or (silhouette == best_silhouette and coherence > best_coherence):
                best_silhouette = silhouette
                best_coherence = coherence
                best_n_topics = n_topics
        except Exception as e:
            print(f"n_topics={n_topics} ê³„ì‚° ì˜¤ë¥˜: {e}")
    print(f"âœ… ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜: {best_n_topics} (ì‹¤ë£¨ì—£: {best_silhouette:.4f}, Coherence: {best_coherence:.4f})")
    return best_n_topics

# í‚¤ì›Œë“œ ì •ì œ í•¨ìˆ˜ (Gemini API)
@lru_cache(maxsize=1000)
def refine_keywords_with_gemini_cached(keywords_tuple, df, model, stopwords, excluded_terms, excluded_bigrams, feature_names, lda):
    topic_id = keywords_tuple[0]
    keywords = list(keywords_tuple[1:])
    
    sample_texts = ' '.join(df[df['topic'] == topic_id]['content'].sample(min(10, len(df[df['topic'] == topic_id])), random_state=42).tolist())[:2000]
    sample_texts = re.sub(r'[{}]', '', sample_texts)
    sample_texts = sample_texts.replace('"', '').replace("'", '').replace('\n', ' ').replace('\r', ' ')
    sample_texts = re.sub(r'\s+', ' ', sample_texts).strip()
    
    prompt = (
        '[í•œêµ­ì–´ ì§€ì‹œì‚¬í•­]\n'
        'ë‹¤ìŒ í‚¤ì›Œë“œ ëª©ë¡ê³¼ ë²•ì•ˆ í…ìŠ¤íŠ¸ ìƒ˜í”Œì„ ë°”íƒ•ìœ¼ë¡œ ë²•ë ¹ ê´€ë ¨ì„±ì„ ê³ ë ¤í•´ ê°€ì¥ ì¤‘ìš”í•œ 4ê°œì˜ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì½¤ë§ˆë¡œ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥:\n'
        '- í‚¤ì›Œë“œëŠ” ì£¼ì–´ì§„ ë²•ì•ˆ í…ìŠ¤íŠ¸ì™€ ì£¼ì œì ìœ¼ë¡œ ì—°ê´€ì„±ì´ ë†’ì•„ì•¼ í•¨\n'
        '- ì¤‘ë³µ/ìœ ì‚¬ì–´ í†µí•©\n'
        '- ì¼ë°˜ì  ë‹¨ì–´ ì œì™¸\n'
        '- ë²•ë ¹ ìš©ì–´ ìš°ì„ ì„ íƒ\n'
        '- ê²°ê³¼ëŠ” ì •í™•íˆ 4ê°œì˜ í‚¤ì›Œë“œë¡œ, ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ë°˜í™˜\n'
        '- ë¶ˆìš©ì–´ ë° ì œì™¸ ë‹¨ì–´, ë°”ì´ê·¸ë¨ì€ í¬í•¨ì‹œí‚¤ì§€ ì•ŠìŒ\n'
        '- ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ë©”íƒ€ ì •ë³´, ì½”ë“œë¸”ë¡, ê¸°íƒ€ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨ì‹œí‚¤ì§€ ì•ŠìŒ\n'
        '- JSON ì´ì™¸ì˜ ë¬¸ìëŠ” 1byteë„ í—ˆìš©ë˜ì§€ ì•ŠìŒ\n\n'
        f'ì›ë³¸ í‚¤ì›Œë“œ: {", ".join(keywords)}\n'
        f'ë²•ì•ˆ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: {sample_texts}'
    )
    
    max_retries = 5
    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt)
            if response and response.text:
                cleaned_response = extract_json_from_codeblock(response.text)
                print(f"ğŸ” Gemini í‚¤ì›Œë“œ ì‘ë‹µ (ì‹œë„ {attempt+1}, í† í”½ {topic_id}): {cleaned_response[:200]}...")
                
                if ',' in cleaned_response:
                    kw_list = [kw.strip() for kw in cleaned_response.split(',') if kw.strip()]
                    filtered_keywords = []
                    for kw in kw_list:
                        if (kw not in stopwords and
                            kw not in excluded_terms and
                            kw not in excluded_bigrams and
                            len(kw) > 1 and
                            not any(part in stopwords or part in excluded_terms for part in kw.split())):
                            filtered_keywords.append(kw)
                        if len(filtered_keywords) >= 4:
                            break
                    if len(filtered_keywords) >= 4:
                        return ", ".join(filtered_keywords[:4])
                
                try:
                    result = json.loads(cleaned_response)
                    if isinstance(result, dict):
                        kw_list = result.get('refined_keywords', [])
                    else:
                        kw_list = result
                    filtered_keywords = [str(kw).strip() for kw in kw_list if kw.strip()]
                    return ", ".join(filtered_keywords[:4])
                except json.JSONDecodeError:
                    pass
                
                filtered_keywords = []
                for kw in keywords:
                    if (kw not in filtered_keywords and
                        kw not in stopwords and
                        kw not in excluded_terms and
                        kw not in excluded_bigrams and
                        len(kw) > 1):
                        filtered_keywords.append(kw)
                    if len(filtered_keywords) >= 4:
                        break
                return ", ".join(filtered_keywords[:4])
            time.sleep(2 ** attempt)
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜ (ì‹œë„ {attempt+1}/{max_retries}, í† í”½ {topic_id}): {str(e)[:100]}")
            time.sleep(2 ** attempt)
    
    filtered_keywords = [
        kw for kw in keywords
        if (kw not in stopwords and
            kw not in excluded_terms and
            kw not in excluded_bigrams and
            len(kw) > 1)
    ]
    if len(filtered_keywords) >= 4:
        return ", ".join(filtered_keywords[:4])
    return ", ".join(feature_names[lda.components_[topic_id].argsort()[::-1][:4]])

# ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜
def refine_keywords_parallel(topic_keywords, df, model, stopwords, excluded_terms, excluded_bigrams, feature_names, lda):
    topic_labels_refined = {}
    with ThreadPoolExecutor(max_workers=4) as executor:
        future_to_topic = {
            executor.submit(
                refine_keywords_with_gemini_cached,
                (topic_id,) + tuple(keywords),
                df, model, stopwords, excluded_terms, excluded_bigrams, feature_names, lda
            ): topic_id
            for topic_id, keywords in topic_keywords.items()
        }
        for future in tqdm(as_completed(future_to_topic), total=len(future_to_topic)):
            topic_id = future_to_topic[future]
            try:
                refined = future.result()
                topic_labels_refined[topic_id] = refined
            except Exception as e:
                print(f"âš ï¸ í† í”½ {topic_id} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)[:100]}")
                topic_labels_refined[topic_id] = ", ".join(feature_names[lda.components_[topic_id].argsort()[::-1][:4]])
    return topic_labels_refined

if __name__ == '__main__':
    from multiprocessing import freeze_support
    freeze_support()

    # ì‹œìŠ¤í…œ ê²½ë¡œ ì„¤ì •
    sys.path.append(str(Path(__file__).resolve().parents[1]))
    import settings

    # Gemini API ì´ˆê¸°í™”
    GEMINI_API_KEY = "AIzaSyA8M00iSzCK1Lvc5YfxamYgQf-Lh4xh5R0"  # ì‹¤ì œ í‚¤ë¡œ êµì²´
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel('gemini-1.5-pro-latest')

    # ë°ì´í„° ë¡œë“œ
    file_path = settings.BASE_DIR / 'geovote' / 'data' / 'bill_filtered_final.csv'
    df = pd.read_csv(file_path, encoding='utf-8-sig')

    # Kiwi ì´ˆê¸°í™”
    kiwi = Kiwi(model_type='knlm')
    custom_nouns = [
        'ëŒ€í†µë ¹ë¹„ì„œì‹¤', 'êµ­ê°€ì•ˆë³´ì‹¤', 'ëŒ€í†µë ¹ê²½í˜¸ì²˜', 'í—Œë²•ìƒëŒ€í†µë ¹ìë¬¸ê¸°êµ¬', 'êµ­ê°€ì•ˆì „ë³´ì¥íšŒì˜',
    'ë¯¼ì£¼í‰í™”í†µì¼ìë¬¸íšŒì˜', 'êµ­ë¯¼ê²½ì œìë¬¸íšŒì˜', 'êµ­ê°€ê³¼í•™ê¸°ìˆ ìë¬¸íšŒì˜', 'ê°ì‚¬ì›', 'êµ­ê°€ì •ë³´ì›',
    'ë°©ì†¡í†µì‹ ìœ„ì›íšŒ', 'íŠ¹ë³„ê°ì°°ê´€', 'ê³ ìœ„ê³µì§ìë²”ì£„ìˆ˜ì‚¬ì²˜', 'êµ­ê°€ì¸ê¶Œìœ„ì›íšŒ', 'êµ­ë¬´ì¡°ì •ì‹¤',
    'êµ­ë¬´ì´ë¦¬ë¹„ì„œì‹¤', 'ì¸ì‚¬í˜ì‹ ì²˜', 'ë²•ì œì²˜', 'ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜', 'ê³µì •ê±°ë˜ìœ„ì›íšŒ',
    'êµ­ë¯¼ê¶Œìµìœ„ì›íšŒ', 'ê¸ˆìœµìœ„ì›íšŒ', 'ê°œì¸ì •ë³´ë³´í˜¸ìœ„ì›íšŒ', 'ì›ìë ¥ì•ˆì „ìœ„ì›íšŒ', 'ê¸°íšì¬ì •ë¶€',
    'êµ­ì„¸ì²­', 'ê´€ì„¸ì²­', 'ì¡°ë‹¬ì²­', 'í†µê³„ì²­', 'êµìœ¡ë¶€', 'ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€', 'ìš°ì£¼í•­ê³µì²­',
    'ì™¸êµë¶€', 'ì¬ì™¸ë™í¬ì²­', 'í†µì¼ë¶€', 'ë²•ë¬´ë¶€', 'ê²€ì°°ì²­', 'êµ­ë°©ë¶€', 'ë³‘ë¬´ì²­', 'ë°©ìœ„ì‚¬ì—…ì²­',
    'í–‰ì •ì•ˆì „ë¶€', 'ê²½ì°°ì²­', 'ì†Œë°©ì²­', 'êµ­ê°€ë³´í›ˆë¶€', 'ë¬¸í™”ì²´ìœ¡ê´€ê´‘ë¶€', 'êµ­ê°€ìœ ì‚°ì²­',
    'ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€', 'ë†ì´Œì§„í¥ì²­', 'ì‚°ë¦¼ì²­', 'ì‚°ì—…í†µìƒìì›ë¶€', 'íŠ¹í—ˆì²­', 'ë³´ê±´ë³µì§€ë¶€',
    'ì§ˆë³‘ê´€ë¦¬ì²­', 'í™˜ê²½ë¶€', 'ê¸°ìƒì²­', 'ê³ ìš©ë…¸ë™ë¶€', 'ì—¬ì„±ê°€ì¡±ë¶€', 'êµ­í† êµí†µë¶€',
    'í–‰ì •ì¤‘ì‹¬ë³µí•©ë„ì‹œê±´ì„¤ì²­', 'ìƒˆë§Œê¸ˆê°œë°œì²­', 'í•´ì–‘ìˆ˜ì‚°ë¶€', 'í•´ì–‘ê²½ì°°ì²­', 'ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€',
    'ìƒì„ìœ„ì›íšŒ', 'ë²•ì œì‚¬ë²•ìœ„ì›íšŒ', 'ì •ë¬´ìœ„ì›íšŒ', 'ê¸°íšì¬ì •ìœ„ì›íšŒ', 'êµìœ¡ìœ„ì›íšŒ',
    'ê³¼í•™ê¸°ìˆ ì •ë³´ë°©ì†¡í†µì‹ ìœ„ì›íšŒ', 'ì™¸êµí†µì¼ìœ„ì›íšŒ', 'êµ­ë°©ìœ„ì›íšŒ', 'í–‰ì •ì•ˆì „ìœ„ì›íšŒ',
    'ë¬¸í™”ì²´ìœ¡ê´€ê´‘ìœ„ì›íšŒ', 'ë†ë¦¼ì¶•ì‚°ì‹í’ˆí•´ì–‘ìˆ˜ì‚°ìœ„ì›íšŒ', 'ì‚°ì—…í†µìƒìì›ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ìœ„ì›íšŒ',
    'ë³´ê±´ë³µì§€ìœ„ì›íšŒ', 'í™˜ê²½ë…¸ë™ìœ„ì›íšŒ', 'êµ­í† êµí†µìœ„ì›íšŒ', 'ì •ë³´ìœ„ì›íšŒ', 'ì—¬ì„±ê°€ì¡±ìœ„ì›íšŒ',
    'ì˜ˆì‚°ê²°ì‚°íŠ¹ë³„ìœ„ì›íšŒ', 'íŠ¹ë³„ìœ„ì›íšŒ', 'ì†Œìœ„ì›íšŒ', 'ë²•ì•ˆì‹¬ì‚¬ì†Œìœ„', 'ì˜ì•ˆ', 'ë²•ë¥ ì•ˆ',
    'ì˜ˆì‚°ì•ˆ', 'ë™ì˜ì•ˆ', 'ìŠ¹ì¸ì•ˆ', 'ê²°ì˜ì•ˆ', 'ê±´ì˜ì•ˆ', 'ê·œì¹™ì•ˆ', 'ì„ ì¶œì•ˆ', 'ë°œì˜', 'ì œì¶œ',
    'ì œì•ˆ', 'ì œì˜', 'ì˜ê²°', 'ë¶€ê²°', 'íê¸°', 'ê°€ê²°', 'ì±„íƒ', 'ì…ë²•ì˜ˆê³ ', 'ê³µí¬', 'ì‹œí–‰',
    'ê°œì •', 'ì œì •', 'íì§€', 'ì¼ë¶€ê°œì •', 'ì „ë¶€ê°œì •',
    'í—Œë²•', 'ë¯¼ë²•', 'í˜•ë²•', 'ìƒë²•', 'í–‰ì •ë²•', 'ë…¸ë™ë²•', 'ì„¸ë²•', 'í™˜ê²½ë²•', 'ì •ë³´í†µì‹ ë²•',
    'ê¸ˆìœµë²•', 'ë³´ê±´ì˜ë£Œë²•', 'êµìœ¡ë²•', 'ë¬¸í™”ì˜ˆìˆ ë²•', 'ë†ë¦¼ë²•', 'ê±´ì„¤ë²•', 'í•´ì–‘ë²•', 'ê¹€ì˜ë€ë²•',
    'ë¶€ì •ì²­íƒê¸ˆì§€ë²•', 'ê³µì§ììœ¤ë¦¬ë²•', 'ì •ì¹˜ìê¸ˆë²•', 'ê³µì§ì„ ê±°ë²•', 'ì „ê¸°í†µì‹ ì‚¬ì—…ë²•',
    'ê°œì¸ì •ë³´ë³´í˜¸ë²•', 'êµ­ê°€ìœ ì‚°ìˆ˜ë¦¬ê¸°ìˆ ìœ„ì›íšŒ', 'ë¶€ê°€ê°€ì¹˜ì„¸ë²•', 'ìˆ˜ì…ì‹í’ˆì•ˆì „ê´€ë¦¬íŠ¹ë³„ë²•',
    'ë‹¤ë¬¸í™”ê°€ì¡±ì§€ì›ë²•',
    'ì¸ê³µì§€ëŠ¥', 'ë¹…ë°ì´í„°', 'ì‚¬ë¬¼ì¸í„°ë„·', 'í´ë¼ìš°ë“œ', 'ë¸”ë¡ì²´ì¸', 'ë©”íƒ€ë²„ìŠ¤', 'ë””ì§€í„¸í”Œë«í¼',
    'ì „ìì •ë¶€', 'ë””ì§€í„¸ì „í™˜', 'ì‚¬ì´ë²„ë³´ì•ˆ', 'ë””ì§€í„¸ë‰´ë”œ', 'ìŠ¤ë§ˆíŠ¸ì‹œí‹°', 'ë””ì§€í„¸í¬ìš©',
    'ì˜¨ë¼ì¸í”Œë«í¼', 'ì „ììƒê±°ë˜',
    'ì½”ë¡œë‚˜19', 'ê°ì—¼ë³‘', 'ë°±ì‹ ', 'ë°©ì—­', 'ì‚¬íšŒì ê±°ë¦¬ë‘ê¸°', 'ì¬ë‚œì§€ì›ê¸ˆ', 'ê¸°í›„ë³€í™”',
    'ë¯¸ì„¸ë¨¼ì§€', 'íê¸°ë¬¼ì²˜ë¦¬', 'ì¬í™œìš©', 'ìˆœí™˜ê²½ì œ', 'ì  ë”í‰ë“±', 'ì„±í¬ë¡±', 'ì„±í­ë ¥', 'ìŠ¤í† í‚¹',
    'ê°€ì •í­ë ¥', 'ë””ì§€í„¸ì„±ë²”ì£„', 'ì²­ë…„ì •ì±…', 'ì²­ë…„ê³ ìš©', 'ì²­ë…„ì£¼íƒ', 'í•™ìê¸ˆëŒ€ì¶œ', 'êµìœ¡ê²©ì°¨',
    ]
    for noun in custom_nouns:
        kiwi.add_user_word(noun, 'NNG', 9.0)

    stopwords = {
    'ì¡°', 'í•­', 'í˜¸', 'ê²½ìš°', 'ë“±', 'ìˆ˜', 'ê²ƒ', 'ì´', 'ì°¨', 'í›„', 'ì´ìƒ', 'ì´í•˜', 'ì´ë‚´',
    'ì•ˆ', 'ì†Œ', 'ëŒ€', 'ì ', 'ê°„', 'ê³³', 'í•´ë‹¹', 'ì°¨', 'ì™¸', 'ê²½ìš°', 'ë‚˜', 'ë°”', 'ì‹œ',
    'ê´€ë ¨', 'ê´€í•˜ì—¬', 'ëŒ€í•˜ì—¬', 'ë”°ë¼', 'ë”°ë¥¸', 'ìœ„í•˜ì—¬', 'ì˜í•˜ì—¬', 'ë•Œ', 'ê°', 'ì', 'ì¸',
    'ë‚´', 'ì¤‘', 'ë•Œë¬¸', 'ìœ„í•´', 'í†µí•´', 'ë¶€í„°', 'ê¹Œì§€', 'ë™ì•ˆ', 'ì‚¬ì´', 'ê¸°ì¤€', 'ë³„ë„',
    'ë³„ì²¨', 'ë³„í‘œ', 'ì œí•œ', 'íŠ¹ì¹™', 'ê°€ëŠ¥', 'ê³¼ì •', 'ê¸°ë°˜', 'ê¸°ì¡´', 'ê·¼ê±°', 'ê¸°ëŠ¥', 'ë°©ì‹',
    'ë²”ìœ„', 'ì‚¬í•­', 'ì‹œì ', 'ìµœê·¼', '?', 'ë…„', 'ì¥', 'í•´', 'ëª…', 'ë‚ ', 'íšŒ',
    'ë™', 'ë°', 'êµ­', 'ë°–', 'ì†', 'ì‹', 'ìœµ', 'ë°–', 'ê·œ', 'í˜„í–‰ë²•', 'ì§', 'ë²”', 'ë§Œ', 'ì…', 'ì§',
    'ì‹ ',
    'ê°€', 'ê°€ë ¹', 'ê°€ì§€', 'ê°ê°', 'ê°ì', 'ê°ì¢…', 'ê°–ê³ ë§í•˜ìë©´', 'ê°™ë‹¤', 'ê°™ì´', 'ê±°ë‹ˆì™€',
    'ê±°ì˜', 'ê²ƒê³¼ ê°™ì´', 'ê²ƒë“¤', 'ê²Œë‹¤ê°€', 'ê²¨ìš°', 'ê²°ê³¼ì— ì´ë¥´ë‹¤', 'ê²°êµ­', 'ê²°ë¡ ì„ ë‚¼ ìˆ˜ ìˆë‹¤',
    'ê²¸ì‚¬ê²¸ì‚¬', 'ê³ ë ¤í•˜ë©´', 'ê³ ë¡œ', 'ê³§', 'ê³¼', 'ê³¼ì—°', 'ê´€ê³„ê°€ ìˆë‹¤', 'ê´€ê³„ì—†ì´', 'ê´€ë ¨ì´ ìˆë‹¤',
    'ê´€í•œ', 'ê´€í•´ì„œëŠ”', 'êµ¬ì²´ì ìœ¼ë¡œ', 'ê·¸', 'ê·¸ë“¤', 'ê·¸ë•Œ', 'ê·¸ë˜', 'ê·¸ë˜ë„', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ¬ë‚˜',
    'ê·¸ëŸ¬ë‹ˆ', 'ê·¸ëŸ¬ë‹ˆê¹Œ', 'ê·¸ëŸ¬ë©´', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ê·¸ëŸ° ê¹Œë‹­ì—', 'ê·¸ëŸ°ë°', 'ê·¸ëŸ¼', 'ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ ',
    'ê·¸ë ‡ê²Œ í•¨ìœ¼ë¡œì¨', 'ê·¸ë ‡ì§€', 'ê·¸ë ‡ì§€ ì•Šë‹¤ë©´', 'ê·¸ë ‡ì§€ ì•Šìœ¼ë©´', 'ê·¸ë ‡ì§€ë§Œ', 'ê·¸ë¦¬ê³ ', 'ê·¸ë¦¬í•˜ì—¬',
    'ê·¸ë§Œì´ë‹¤', 'ê·¸ì— ë”°ë¥´ëŠ”', 'ê·¸ì €', 'ê·¼ê±°ë¡œ', 'ê¸°íƒ€', 'ê¹Œì§€', 'ê¹Œì§€ë„', 'ë‚˜', 'ë‚˜ë¨¸ì§€ëŠ”', 'ë‚¨ë“¤',
    'ë„ˆ', 'ë„ˆí¬', 'ë„¤', 'ë…¼í•˜ì§€ ì•Šë‹¤', 'ë‹¤ë¥¸', 'ë‹¤ë§Œ', 'ë‹¤ì†Œ', 'ë‹¤ìˆ˜', 'ë‹¤ì‹œ ë§í•˜ìë©´', 'ë‹¤ìŒ',
    'ë‹¤ìŒì—', 'ë‹¤ìŒìœ¼ë¡œ', 'ë‹¨ì§€', 'ë‹¹ì‹ ', 'ëŒ€í•˜ë©´', 'ëŒ€í•´ ë§í•˜ìë©´', 'ëŒ€í•´ì„œ', 'ë”êµ¬ë‚˜', 'ë”êµ°ë‹¤ë‚˜',
    'ë”ë¼ë„', 'ë”ë¶ˆì–´', 'ë™ì‹œì—', 'ëœë°”ì—ì•¼', 'ëœì´ìƒ', 'ë‘ë²ˆì§¸ë¡œ', 'ë‘˜', 'ë“±ë“±', 'ë”°ë¼', 'ï¿½ yawn',
    'ë”°ì§€ì§€ ì•Šë‹¤', 'ë•Œê°€ ë˜ì–´', 'ë˜', 'ë˜í•œ', 'ë¼ í•´ë„', 'ë ¹', 'ë¡œ', 'ë¡œ ì¸í•˜ì—¬', 'ë¡œë¶€í„°', 'ë¥¼',
    'ë§ˆìŒëŒ€ë¡œ', 'ë§ˆì €', 'ë§‰ë¡ í•˜ê³ ', 'ë§Œ ëª»í•˜ë‹¤', 'ë§Œì•½', 'ë§Œì•½ì—', 'ë§Œì€ ì•„ë‹ˆë‹¤', 'ë§Œì¼', 'ë§Œí¼',
    'ë§í•˜ìë©´', 'ë§¤', 'ëª¨ë‘', 'ë¬´ë µ', 'ë¬´ìŠ¨', 'ë¬¼ë¡ ', 'ë°', 'ë°”ê¾¸ì–´ë§í•˜ë©´', 'ë°”ê¾¸ì–´ì„œ ë§í•˜ë©´',
    'ë°”ë¡œ', 'ë°–ì— ì•ˆëœë‹¤', 'ë°˜ëŒ€ë¡œ', 'ë°˜ë“œì‹œ', 'ë²„ê¸ˆ', 'ë³´ëŠ”ë°ì„œ', 'ë³¸ëŒ€ë¡œ', 'ë¶€ë¥˜ì˜ ì‚¬ëŒë“¤',
    'ë¶ˆêµ¬í•˜ê³ ', 'ì‚¬', 'ì‚¼', 'ìƒëŒ€ì ìœ¼ë¡œ ë§í•˜ìë©´', 'ì„¤ë ¹', 'ì„¤ë§ˆ', 'ì…‹', 'ì†Œìƒ', 'ìˆ˜', 'ìŠµë‹ˆê¹Œ',
    'ìŠµë‹ˆë‹¤', 'ì‹œê°„', 'ì‹œì‘í•˜ì—¬', 'ì‹¤ë¡œ', 'ì‹¬ì§€ì–´', 'ì•„', 'ì•„ë‹ˆ', 'ì•„ë‹ˆë¼ë©´', 'ì•„ë‹ˆë©´', 'ì•„ì•¼',
    'ì•„ìš¸ëŸ¬', 'ì•ˆ ê·¸ëŸ¬ë©´', 'ì•Šê¸° ìœ„í•˜ì—¬', 'ì•Œ ìˆ˜ ìˆë‹¤', 'ì•ì—ì„œ', 'ì•½ê°„', 'ì–‘ì', 'ì–´', 'ì–´ëŠ',
    'ì–´ë””', 'ì–´ë•Œ', 'ì–´ë– í•œ', 'ì–´ë–¤', 'ì–´ë–»ê²Œ', 'ì–´ì§¸ì„œ', 'ì–´ì¨‹ë“ ', 'ì–¸ì œ', 'ì–¼ë§ˆ', 'ì–¼ë§ˆë§Œí¼',
    'ì—‰ì—‰', 'ì—', 'ì—ê²Œ', 'ì—ì„œ', 'ì—ì´', 'ì—”', 'ì˜', 'ì˜ˆ', 'ì˜¬', 'ì™€', 'ì™€ë¥´ë¥´', 'ì™€ì•„', 'ì™œ',
    'ì™¸ì—', 'ìš”', 'ìš°ë¦¬', 'ì›', 'ì›”', 'ìœ„ì—ì„œ ì„œìˆ í•œë°”ì™€ê°™ì´', 'ìœ™ìœ™', 'ìœ¡', 'ìœ¼ë¡œ', 'ìœ¼ë¡œì„œ',
    'ìœ¼ë¡œì¨', 'ì„', 'ì‘', 'ì‘ë‹¹', 'ì˜', 'ì˜ê±°í•˜ì—¬', 'ì˜ì§€í•˜ì—¬', 'ì˜í•´', 'ì´', 'ì´ë¥´ë‹¤', 'ì´ìª½',
    'ì¸ì  ', 'ì¼', 'ì¼ê²ƒì´ë‹¤', 'ì„ì— í‹€ë¦¼ì—†ë‹¤', 'ìê¸°', 'ìê¸°ì§‘', 'ìë§ˆì', 'ìì‹ ', 'ì ê¹', 'ì ì‹œ',
    'ì €', 'ì €ê¸°', 'ì €ìª½', 'ì „ë¶€', 'ì „ì', 'ì •ë„ì— ì´ë¥´ë‹¤', 'ì œ', 'ì œì™¸í•˜ê³ ', 'ì¡°ì°¨', 'ì¢‹ì•„', 'ì¢ì¢',
    'ì£¼', 'ì£¼ì €í•˜ì§€ ì•Šê³ ', 'ì¤„', 'ì¤‘ì´ë‹¤', 'ì¦ˆìŒ', 'ì¦‰', 'ì§€ê¸ˆ', 'ì§€ë§ê³ ', 'ì§„ì§œë¡œ', 'ìª½ìœ¼ë¡œ',
    'ì¯¤', 'ì°¨ë¼ë¦¬', 'ì°¸', 'ì²«ë²ˆì§¸ë¡œ', 'ì³‡', 'ì½¸ì½¸', 'ì¾…ì¾…', 'ì¿µ', 'í¼', 'íƒ€ë‹¤', 'í†µí•˜ë‹¤', 'í‹ˆíƒ€',
    'íŒ', 'í½', 'í•˜', 'í•˜ê²Œë ê²ƒì´ë‹¤', 'í•˜ê²Œí•˜ë‹¤', 'í•˜ê² ëŠ”ê°€', 'í•˜ê³ ', 'í•˜ê³ ìˆì—ˆë‹¤', 'í•˜ê³¤í•˜ì˜€ë‹¤',
    'í•˜êµ¬ë‚˜', 'í•˜ê¸° ë•Œë¬¸ì—', 'í•˜ê¸°ë§Œ í•˜ë©´', 'í•˜ê¸°ë³´ë‹¤ëŠ”', 'í•˜ê¸°ì—', 'í•˜ë‚˜', 'í•˜ëŠë‹ˆ', 'í•˜ëŠ” ê²ƒë§Œ',
    'í•˜ëŠ” í¸ì´ ë‚«ë‹¤', 'í•˜ëŠ”ê²ƒë„', 'í•˜ë”ë¼ë„', 'í•˜ë„ë‹¤', 'í•˜ë„ë¡ì‹œí‚¤ë‹¤', 'í•˜ë„ë¡í•˜ë‹¤', 'í•˜ë“ ì§€',
    'í•˜ë ¤ê³ í•˜ë‹¤', 'í•˜ë§ˆí„°ë©´', 'í•˜ë©´ í• ìˆ˜ë¡', 'í•˜ë©´ì„œ', 'í•˜ë¬¼ë©°', 'í•˜ì—¬ê¸ˆ', 'í•˜ì—¬ì•¼', 'í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´',
    'í•˜ì§€ ì•Šë„ë¡', 'í•˜ì§€ë§ˆ', 'í•˜ì§€ë§ˆë¼', 'í•˜ì²œ', 'í•˜í’ˆ', 'í•œ', 'í•œ ì´ìœ ëŠ”', 'í•œ í›„', 'í•œë‹¤ë©´',
    'í•œë‹¤ë©´ ëª°ë¼ë„', 'í•œë°', 'í•œë§ˆë””', 'í•œì ì´ìˆë‹¤', 'í•œì¼ ìœ¼ë¡œëŠ”', 'í•œí¸', 'í•  ë”°ë¦„ì´ë‹¤', 'í•  ìƒê°ì´ë‹¤',
    'í•  ì¤„ ì•ˆë‹¤', 'í•  ì§€ê²½ì´ë‹¤', 'í• ë•Œ', 'í• ë§Œí•˜ë‹¤', 'í• ë§ì •', 'í• ë¿', 'í•¨ê»˜', 'í•´ë„',
    'í•´ë´ìš”', 'í•´ì„œëŠ”', 'í•´ì•¼í•œë‹¤', 'í•´ìš”', 'í–ˆì–´ìš”', 'í˜•ì‹ìœ¼ë¡œ ì“°ì—¬', 'í˜¹ì‹œ', 'í˜¹ì€', 'í˜¼ì', 'í›¨ì”¬',
    'íœ˜ìµ', 'íœ´', 'íí', 'í˜ì…ì–´',
    'ê°€.', 'ë‚˜.', 'ë‹¤.', 'ë¼.', 'ë§ˆ.', 'ë°”.', 'ì‚¬.', 'ì•„.', 'ì.', 'ì°¨.', 'ì¹´.', 'íƒ€.', 'íŒŒ.', 'í•˜.',
    '1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.', '10.', '11.', '12.', '13.', '14.', '15.',
    'â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤', 'â‘¥', 'â‘¦', 'â‘§', 'â‘¨', 'â‘©',
    'â… ', 'â…¡', 'â…¢', 'â…£', 'â…¤', 'â…¥', 'â…¦', 'â…§', 'â…¨', 'â…©',
    'i.', 'ii.', 'iii.', 'iv.', 'v.', 'vi.', 'vii.', 'viii.', 'ix.', 'x.',
    'ë²•ë¥ ', 'ë²•', 'ì¡°ë¡€', 'ê·œì •', 'ì¡°í•­', 'ì¡°ë¬¸', 'ì¡°ì¹˜', 'ì¡°ì •', 'ê·œì¹™',
    'ë²•ì•ˆ', 'ì…ë²•', 'ê°œì •', 'ì œì •', 'ì‹œí–‰', 'ê³µí¬', 'íì§€', 'ì¼ë¶€ê°œì •', 'ì „ë¶€ê°œì •', 'ë™ì˜ì•ˆ', 'ìŠ¹ì¸ì•ˆ', 'ê²°ì˜ì•ˆ', 'ê±´ì˜ì•ˆ', 'ê·œì¹™ì•ˆ', 'ì„ ì¶œì•ˆ',
    'ë°œì˜', 'ì œì¶œ', 'ì œì•ˆ', 'ì œì˜', 'ì˜ê²°', 'ë¶€ê²°', 'íê¸°', 'ê°€ê²°', 'ì±„íƒ',
    'ì…ë²•ì˜ˆê³ ', 'ì²œë§Œ', 'ê¸°ê´€', 'ê¸°ê°„'
}

    # ì´ˆê¸° ì„¤ì • (frozenset ì‚¬ìš©)
    initial_stopwords = frozenset({
        'ì¡°', 'í•­', 'í˜¸', 'ê²½ìš°', 'ë“±', 'ìˆ˜', 'ê²ƒ', 'ì´', 'ì°¨', 'í›„', 'ì´ìƒ', 'ì´í•˜', 'ì´ë‚´',
        'ì•ˆ', 'ì†Œ', 'ëŒ€', 'ì ', 'ê°„', 'ê³³', 'í•´ë‹¹', 'ì™¸', 'ë‚˜', 'ë°”', 'ì‹œ', 'ê´€ë ¨', 'ê´€í•˜ì—¬',
        'ëŒ€í•˜ì—¬', 'ë”°ë¼', 'ë”°ë¥¸', 'ìœ„í•˜ì—¬', 'ì˜í•˜ì—¬', 'ë•Œ', 'ê°', 'ì', 'ì¸', 'ë‚´', 'ì¤‘',
        'ë•Œë¬¸', 'ìœ„í•´', 'í†µí•´', 'ë¶€í„°', 'ê¹Œì§€', 'ë™ì•ˆ', 'ì‚¬ì´', 'ê¸°ì¤€', 'ë³„ë„', 'ë³„ì²¨', 'ë³„í‘œ',
        'ì œí•œ', 'íŠ¹ì¹™', 'ê°€ëŠ¥', 'ê³¼ì •', 'ê¸°ë°˜', 'ê¸°ì¡´', 'ê·¼ê±°', 'ê¸°ëŠ¥', 'ë°©ì‹', 'ë²”ìœ„', 'ì‚¬í•­',
        'ì‹œì ', 'ìµœê·¼', 'ë…„', 'ì¥', 'í•´', 'ëª…', 'ë‚ ', 'íšŒ', 'ë™', 'ë°', 'êµ­', 'ë°–', 'ì†', 'ì‹',
        'ê·œ', 'í˜„í–‰ë²•', 'ì§', 'ë²”', 'ë§Œ', 'ì…', 'ì‹ ',
    })
    initial_excluded_terms = frozenset({
        'ì£¼ìš”', 'ìˆ˜ì‚¬', 'ê´€ë ¨', 'ì‚¬í•­', 'ì •ì±…', 'ëŒ€ìƒ', 'ë°©ì•ˆ', 'ì¶”ì§„', 'ê°•í™”', 'ê°œì„ ', 'ì§€ì›',
        'í™•ëŒ€', 'ì¡°ì¹˜', 'í•„ìš”', 'í˜„í™©', 'ê¸°ë°˜', 'ê³¼ì •', 'ê¸°ì¡´', 'ê·¼ê±°', 'ê¸°ëŠ¥', 'ë°©ì‹', 'ë²”ìœ„',
        'í™œë™', 'ìš´ì˜', 'ê´€ë¦¬', 'ì‹¤ì‹œ', 'í™•ë³´', 'êµ¬ì„±', 'ì„¤ì¹˜', 'ì§€ì •', 'ê³„íš', 'ìˆ˜ë¦½',
    })
    preserve_terms = frozenset({'ë²•ë¥ ', 'ë²•ì•ˆ', 'ì…ë²•', 'ê°œì •', 'ì œì •', 'ì‹œí–‰', 'ê³µí¬', 'íì§€', 'ì¡°ë¡€', 'ê·œì •', 'ì¡°í•­', 'ì˜ê²°'})
    excluded_bigrams = frozenset({'êµìœ¡ ì‹¤ì‹œ', 'ì§•ì—­ ë²Œê¸ˆ', 'ìˆ˜ë¦½ ì‹œí–‰', 'ìš´ì˜ ê´€ë¦¬'})

    # ìƒ˜í”Œë§ ë° ì •ì œ
    sample_size = min(100, len(df))
    sample_texts = ' '.join(df['content'].sample(sample_size, random_state=42).tolist())
    stopwords, excluded_terms = refine_stopwords_excluded_terms(
        sample_texts, model, initial_stopwords, initial_excluded_terms, preserve_terms
    )
    stopwords = set(stopwords).union(initial_stopwords)
    excluded_terms = set(excluded_terms).union(initial_excluded_terms)

    # ì „ì²˜ë¦¬
    df['content_processed'] = extract_tokens(df['content'].tolist(), kiwi, excluded_terms, stopwords)

    # TF-IDF ë²¡í„°í™”
    vectorizer = TfidfVectorizer(
        max_df=0.7,
        min_df=5,
        ngram_range=(1,3),
        max_features=10000,
        token_pattern=r"(?u)\b\w+\b"
    )
    X = vectorizer.fit_transform(df['content_processed'])

    # LDA ëª¨ë¸ í›ˆë ¨
    texts = [doc.split() for doc in df['content_processed'] if doc]
    dictionary = Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]
    
    n_topics = find_optimal_n_topics(X, texts, dictionary, corpus, vectorizer)
    lda = LatentDirichletAllocation(
        n_components=n_topics,
        max_iter=20,
        learning_method='batch',
        random_state=42,
        n_jobs=1
    )
    lda.fit(X)
    df['topic'] = lda.transform(X).argmax(axis=1)

    # í‚¤ì›Œë“œ ì •ì œ
    feature_names = vectorizer.get_feature_names_out()
    topic_keywords = {}
    for topic_idx, topic in enumerate(lda.components_):
        top_indices = topic.argsort()[::-1][:12]
        keywords = [
            feature_names[i] for i in top_indices
            if (feature_names[i] not in stopwords and
                feature_names[i] not in excluded_terms and
                len(feature_names[i]) > 1)
        ]
        topic_keywords[topic_idx] = keywords[:6]

    print("\nğŸ”§ Gemini APIë¥¼ ì´ìš©í•œ í‚¤ì›Œë“œ ì •ì œ ì‹œì‘...")
    topic_labels_refined = refine_keywords_parallel(
        topic_keywords, df, model, stopwords, excluded_terms, excluded_bigrams, feature_names, lda
    )

    # ê²°ê³¼ ë§¤í•‘
    df['topic_label'] = df['topic'].map(topic_labels_refined)
    df['topic_label'] = df['topic_label'].fillna("ê¸°íƒ€")

    # ì €ì¥
    output_path = settings.BASE_DIR / 'keword_clustering' / 'data' / 'bill_keyword_clustering_refined2.csv'
    df[['age', 'title', 'bill_id', 'bill_number', 'content', 'topic', 'topic_label']].to_csv(
        output_path,
        index=False,
        quoting=csv.QUOTE_NONNUMERIC,
        encoding='utf-8-sig'
    )
    print("âœ… ì²˜ë¦¬ ì™„ë£Œ! íŒŒì¼:", output_path)
