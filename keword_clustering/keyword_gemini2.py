# -*- coding: utf-8 -*-
import sys
from pathlib import Path
import pandas as pd
from kiwipiepy import Kiwi
from tqdm import tqdm
import numpy as np
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.metrics import silhouette_score
import google.generativeai as genai
import time
import json
import csv
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache

tqdm.pandas()

# ì‹œìŠ¤í…œ ê²½ë¡œ ì„¤ì •
sys.path.append(str(Path(__file__).resolve().parents[1]))
import settings

# Gemini API ì´ˆê¸°í™”
GEMINI_API_KEY = "AIzaSyA8M00iSzCK1Lvc5YfxamYgQf-Lh4xh5R0"
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-1.5-pro-latest')

# ë°ì´í„° ë¡œë“œ
file_path = settings.BASE_DIR / 'geovote' / 'data' / 'bill_filtered_final.csv'
df = pd.read_csv(file_path, encoding='utf-8-sig')

# Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
kiwi = Kiwi(model_type='knlm')

# ì‚¬ìš©ì ì •ì˜ ëª…ì‚¬ ëª©ë¡
custom_nouns = [
    'ëŒ€í†µë ¹ë¹„ì„œì‹¤', 'êµ­ê°€ì•ˆë³´ì‹¤', 'ëŒ€í†µë ¹ê²½í˜¸ì²˜', 'í—Œë²•ìƒëŒ€í†µë ¹ìë¬¸ê¸°êµ¬', 'êµ­ê°€ì•ˆì „ë³´ì¥íšŒì˜',
    'ë¯¼ì£¼í‰í™”í†µì¼ìë¬¸íšŒì˜', 'êµ­ë¯¼ê²½ì œìë¬¸íšŒì˜', 'êµ­ê°€ê³¼í•™ê¸°ìˆ ìë¬¸íšŒì˜', 'ê°ì‚¬ì›', 'êµ­ê°€ì •ë³´ì›',
    'ë°©ì†¡í†µì‹ ìœ„ì›íšŒ', 'íŠ¹ë³„ê°ì°°ê´€', 'ê³ ìœ„ê³µì§ìë²”ì£„ìˆ˜ì‚¬ì²˜', 'êµ­ê°€ì¸ê¶Œìœ„ì›íšŒ', 'êµ­ë¬´ì¡°ì •ì‹¤',
    'êµ­ë¬´ì´ë¦¬ë¹„ì„œì‹¤', 'ì¸ì‚¬í˜ì‹ ì²˜', 'ë²•ì œì²˜', 'ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜', 'ê³µì •ê±°ë˜ìœ„ì›íšŒ',
    'êµ­ë¯¼ê¶Œìµìœ„ì›íšŒ', 'ê¸ˆìœµìœ„ì›íšŒ', 'ê°œì¸ì •ë³´ë³´í˜¸ìœ„ì›íšŒ', 'ì›ìë ¥ì•ˆì „ìœ„ì›íšŒ', 'ê¸°íšì¬ì •ë¶€',
    'êµ­ì„¸ì²­', 'ê´€ì„¸ì²­', 'ì¡°ë‹¬ì²­', 'í†µê³„ì²­', 'êµìœ¡ë¶€', 'ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€', 'ìš°ì£¼í•­ê³µì²­',
    'ì™¸êµë¶€', 'ì¬ì™¸ë™í¬ì²­', 'í†µì¼ë¶€', 'ë²•ë¬´ë¶€', 'ê²€ì°°ì²­', 'êµ­ë°©ë¶€', 'ë³‘ë¬´ì²­', 'ë°©ìœ„ì‚¬ì—…ì²­',
    'í–‰ì •ì•ˆì „ë¶€', 'ê²½ì°°ì²­', 'ì†Œë°©ì²­', 'êµ­ê°€ë³´í›ˆë¶€', 'ë¬¸í™”ì²´ìœ¡ê´€ê´‘ë¶€', 'êµ­ê°€ìœ ì‚°ì²­',
    'ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€', 'ë†ì´Œì§„í¥ì²­', 'ì‚°ë¦¼ì²­', 'ì‚°ì—…í†µìƒìì›ë¶€', 'íŠ¹í—ˆì²­', 'ë³´ê±´ë³µì§€ë¶€',
    'ì§ˆë³‘ê´€ë¦¬ì²­', 'í™˜ê²½ë¶€', 'ê¸°ìƒì²­', 'ê³ ìš©ë…¸ë™ë¶€', 'ì—¬ì„±ê°€ì¡±ë¶€', 'êµ­í† êµí†µë¶€',
    'í–‰ì •ì¤‘ì‹¬ë³µí•©ë„ì‹œê±´ì„¤ì²­', 'ìƒˆë§Œê¸ˆê°œë°œì²­', 'í•´ì–‘ìˆ˜ì‚°ë¶€', 'í•´ì–‘ê²½ì°°ì²­', 'ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€',
    'ìƒì„ìœ„ì›íšŒ', 'ë²•ì œì‚¬ë²•ìœ„ì›íšŒ', 'ì •ë¬´ìœ„ì›íšŒ', 'ê¸°íšì¬ì •ìœ„ì›íšŒ', 'êµìœ¡ìœ„ì›íšŒ',
    'ê³¼í•™ê¸°ìˆ ì •ë³´ë°©ì†¡í†µì‹ ìœ„ì›íšŒ', 'ì™¸êµí†µì¼ìœ„ì›íšŒ', 'êµ­ë°©ìœ„ì›íšŒ', 'í–‰ì •ì•ˆì „ìœ„ì›íšŒ',
    'ë¬¸í™”ì²´ìœ¡ê´€ê´‘ìœ„ì›íšŒ', 'ë†ë¦¼ì¶•ì‚°ì‹í’ˆí•´ì–‘ìˆ˜ì‚°ìœ„ì›íšŒ', 'ì‚°ì—…í†µìƒìì›ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ìœ„ì›íšŒ',
    'ë³´ê±´ë³µì§€ìœ„ì›íšŒ', 'í™˜ê²½ë…¸ë™ìœ„ì›íšŒ', 'êµ­í† êµí†µìœ„ì›íšŒ', 'ì •ë³´ìœ„ì›íšŒ', 'ì—¬ì„±ê°€ì¡±ìœ„ì›íšŒ',
    'ì˜ˆì‚°ê²°ì‚°íŠ¹ë³„ìœ„ì›íšŒ', 'íŠ¹ë³„ìœ„ì›íšŒ', 'ì†Œìœ„ì›íšŒ', 'ë²•ì•ˆì‹¬ì‚¬ì†Œìœ„', 'ì˜ì•ˆ', 'ë²•ë¥ ì•ˆ',
    'ì˜ˆì‚°ì•ˆ', 'ë™ì˜ì•ˆ', 'ìŠ¹ì¸ì•ˆ', 'ê²°ì˜ì•ˆ', 'ê±´ì˜ì•ˆ', 'ê·œì¹™ì•ˆ', 'ì„ ì¶œì•ˆ', 'ë°œì˜', 'ì œì¶œ',
    'ì œì•ˆ', 'ì œì˜', 'ì˜ê²°', 'ë¶€ê²°', 'íê¸°', 'ê°€ê²°', 'ì±„íƒ', 'ì…ë²•ì˜ˆê³ ', 'ê³µí¬', 'ì‹œí–‰',
    'ê°œì •', 'ì œì •', 'íì§€', 'ì¼ë¶€ê°œì •', 'ì „ë¶€ê°œì •',
    'í—Œë²•', 'ë¯¼ë²•', 'í˜•ë²•', 'ìƒë²•', 'í–‰ì •ë²•', 'ë…¸ë™ë²•', 'ì„¸ë²•', 'í™˜ê²½ë²•', 'ì •ë³´í†µì‹ ë²•',
    'ê¸ˆìœµë²•', 'ë³´ê±´ì˜ë£Œë²•', 'êµìœ¡ë²•', 'ë¬¸í™”ì˜ˆìˆ ë²•', 'ë†ë¦¼ë²•', 'ê±´ì„¤ë²•', 'í•´ì–‘ë²•', 'ê¹€ì˜ë€ë²•',
    'ë¶€ì •ì²­íƒê¸ˆì§€ë²•', 'ê³µì§ììœ¤ë¦¬ë²•', 'ì •ì¹˜ìê¸ˆë²•', 'ê³µì§ì„ ê±°ë²•', 'ì „ê¸°í†µì‹ ì‚¬ì—…ë²•',
    'ê°œì¸ì •ë³´ë³´í˜¸ë²•', 'êµ­ê°€ìœ ì‚°ìˆ˜ë¦¬ê¸°ìˆ ìœ„ì›íšŒ', 'ë¶€ê°€ê°€ì¹˜ì„¸ë²•', 'ìˆ˜ì…ì‹í’ˆì•ˆì „ê´€ë¦¬íŠ¹ë³„ë²•',
    'ë‹¤ë¬¸í™”ê°€ì¡±ì§€ì›ë²•',
    'ì¸ê³µì§€ëŠ¥', 'ë¹…ë°ì´í„°', 'ì‚¬ë¬¼ì¸í„°ë„·', 'í´ë¼ìš°ë“œ', 'ë¸”ë¡ì²´ì¸', 'ë©”íƒ€ë²„ìŠ¤', 'ë””ì§€í„¸í”Œë«í¼',
    'ì „ìì •ë¶€', 'ë””ì§€í„¸ì „í™˜', 'ì‚¬ì´ë²„ë³´ì•ˆ', 'ë””ì§€í„¸ë‰´ë”œ', 'ìŠ¤ë§ˆíŠ¸ì‹œí‹°', 'ë””ì§€í„¸í¬ìš©',
    'ì˜¨ë¼ì¸í”Œë«í¼', 'ì „ììƒê±°ë˜',
    'ì½”ë¡œë‚˜19', 'ê°ì—¼ë³‘', 'ë°±ì‹ ', 'ë°©ì—­', 'ì‚¬íšŒì ê±°ë¦¬ë‘ê¸°', 'ì¬ë‚œì§€ì›ê¸ˆ', 'ê¸°í›„ë³€í™”',
    'ë¯¸ì„¸ë¨¼ì§€', 'íê¸°ë¬¼ì²˜ë¦¬', 'ì¬í™œìš©', 'ìˆœí™˜ê²½ì œ', 'ì  ë”í‰ë“±', 'ì„±í¬ë¡±', 'ì„±í­ë ¥', 'ìŠ¤í† í‚¹',
    'ê°€ì •í­ë ¥', 'ë””ì§€í„¸ì„±ë²”ì£„', 'ì²­ë…„ì •ì±…', 'ì²­ë…„ê³ ìš©', 'ì²­ë…„ì£¼íƒ', 'í•™ìê¸ˆëŒ€ì¶œ', 'êµìœ¡ê²©ì°¨',
]
for noun in custom_nouns:
    kiwi.add_user_word(noun, 'NNG', 9.0)

# ë¶ˆìš©ì–´ ë° ì œì™¸ ë‹¨ì–´ ì •ì˜
stopwords = {
    'ì¡°', 'í•­', 'í˜¸', 'ê²½ìš°', 'ë“±', 'ìˆ˜', 'ê²ƒ', 'ì´', 'ì°¨', 'í›„', 'ì´ìƒ', 'ì´í•˜', 'ì´ë‚´',
    'ì•ˆ', 'ì†Œ', 'ëŒ€', 'ì ', 'ê°„', 'ê³³', 'í•´ë‹¹', 'ì°¨', 'ì™¸', 'ê²½ìš°', 'ë‚˜', 'ë°”', 'ì‹œ',
    'ê´€ë ¨', 'ê´€í•˜ì—¬', 'ëŒ€í•˜ì—¬', 'ë”°ë¼', 'ë”°ë¥¸', 'ìœ„í•˜ì—¬', 'ì˜í•˜ì—¬', 'ë•Œ', 'ê°', 'ì', 'ì¸',
    'ë‚´', 'ì¤‘', 'ë•Œë¬¸', 'ìœ„í•´', 'í†µí•´', 'ë¶€í„°', 'ê¹Œì§€', 'ë™ì•ˆ', 'ì‚¬ì´', 'ê¸°ì¤€', 'ë³„ë„',
    'ë³„ì²¨', 'ë³„í‘œ', 'ì œí•œ', 'íŠ¹ì¹™', 'ê°€ëŠ¥', 'ê³¼ì •', 'ê¸°ë°˜', 'ê¸°ì¡´', 'ê·¼ê±°', 'ê¸°ëŠ¥', 'ë°©ì‹',
    'ë²”ìœ„', 'ì‚¬í•­', 'ì‹œì ', 'ìµœê·¼', '?', 'ë…„', 'ì¥', 'í•´', 'ëª…', 'ë‚ ', 'íšŒ',
    'ë™', 'ë°', 'êµ­', 'ë°–', 'ì†', 'ì‹', 'ìœµ', 'ë°–', 'ê·œ', 'í˜„í–‰ë²•', 'ì§', 'ë²”', 'ë§Œ', 'ì…', 'ì§',
    'ì‹ ',
    'ê°€', 'ê°€ë ¹', 'ê°€ì§€', 'ê°ê°', 'ê°ì', 'ê°ì¢…', 'ê°–ê³ ë§í•˜ìë©´', 'ê°™ë‹¤', 'ê°™ì´', 'ê±°ë‹ˆì™€',
    'ê±°ì˜', 'ê²ƒê³¼ ê°™ì´', 'ê²ƒë“¤', 'ê²Œë‹¤ê°€', 'ê²¨ìš°', 'ê²°ê³¼ì— ì´ë¥´ë‹¤', 'ê²°êµ­', 'ê²°ë¡ ì„ ë‚¼ ìˆ˜ ìˆë‹¤',
    'ê²¸ì‚¬ê²¸ì‚¬', 'ê³ ë ¤í•˜ë©´', 'ê³ ë¡œ', 'ê³§', 'ê³¼', 'ê³¼ì—°', 'ê´€ê³„ê°€ ìˆë‹¤', 'ê´€ê³„ì—†ì´', 'ê´€ë ¨ì´ ìˆë‹¤',
    'ê´€í•œ', 'ê´€í•´ì„œëŠ”', 'êµ¬ì²´ì ìœ¼ë¡œ', 'ê·¸', 'ê·¸ë“¤', 'ê·¸ë•Œ', 'ê·¸ë˜', 'ê·¸ë˜ë„', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ¬ë‚˜',
    'ê·¸ëŸ¬ë‹ˆ', 'ê·¸ëŸ¬ë‹ˆê¹Œ', 'ê·¸ëŸ¬ë©´', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ê·¸ëŸ° ê¹Œë‹­ì—', 'ê·¸ëŸ°ë°', 'ê·¸ëŸ¼', 'ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ ',
    'ê·¸ë ‡ê²Œ í•¨ìœ¼ë¡œì¨', 'ê·¸ë ‡ì§€', 'ê·¸ë ‡ì§€ ì•Šë‹¤ë©´', 'ê·¸ë ‡ì§€ ì•Šìœ¼ë©´', 'ê·¸ë ‡ì§€ë§Œ', 'ê·¸ë¦¬ê³ ', 'ê·¸ë¦¬í•˜ì—¬',
    'ê·¸ë§Œì´ë‹¤', 'ê·¸ì— ë”°ë¥´ëŠ”', 'ê·¸ì €', 'ê·¼ê±°ë¡œ', 'ê¸°íƒ€', 'ê¹Œì§€', 'ê¹Œì§€ë„', 'ë‚˜', 'ë‚˜ë¨¸ì§€ëŠ”', 'ë‚¨ë“¤',
    'ë„ˆ', 'ë„ˆí¬', 'ë„¤', 'ë…¼í•˜ì§€ ì•Šë‹¤', 'ë‹¤ë¥¸', 'ë‹¤ë§Œ', 'ë‹¤ì†Œ', 'ë‹¤ìˆ˜', 'ë‹¤ì‹œ ë§í•˜ìë©´', 'ë‹¤ìŒ',
    'ë‹¤ìŒì—', 'ë‹¤ìŒìœ¼ë¡œ', 'ë‹¨ì§€', 'ë‹¹ì‹ ', 'ëŒ€í•˜ë©´', 'ëŒ€í•´ ë§í•˜ìë©´', 'ëŒ€í•´ì„œ', 'ë”êµ¬ë‚˜', 'ë”êµ°ë‹¤ë‚˜',
    'ë”ë¼ë„', 'ë”ë¶ˆì–´', 'ë™ì‹œì—', 'ëœë°”ì—ì•¼', 'ëœì´ìƒ', 'ë‘ë²ˆì§¸ë¡œ', 'ë‘˜', 'ë“±ë“±', 'ë”°ë¼', 'ï¿½ yawn',
    'ë”°ì§€ì§€ ì•Šë‹¤', 'ë•Œê°€ ë˜ì–´', 'ë˜', 'ë˜í•œ', 'ë¼ í•´ë„', 'ë ¹', 'ë¡œ', 'ë¡œ ì¸í•˜ì—¬', 'ë¡œë¶€í„°', 'ë¥¼',
    'ë§ˆìŒëŒ€ë¡œ', 'ë§ˆì €', 'ë§‰ë¡ í•˜ê³ ', 'ë§Œ ëª»í•˜ë‹¤', 'ë§Œì•½', 'ë§Œì•½ì—', 'ë§Œì€ ì•„ë‹ˆë‹¤', 'ë§Œì¼', 'ë§Œí¼',
    'ë§í•˜ìë©´', 'ë§¤', 'ëª¨ë‘', 'ë¬´ë µ', 'ë¬´ìŠ¨', 'ë¬¼ë¡ ', 'ë°', 'ë°”ê¾¸ì–´ë§í•˜ë©´', 'ë°”ê¾¸ì–´ì„œ ë§í•˜ë©´',
    'ë°”ë¡œ', 'ë°–ì— ì•ˆëœë‹¤', 'ë°˜ëŒ€ë¡œ', 'ë°˜ë“œì‹œ', 'ë²„ê¸ˆ', 'ë³´ëŠ”ë°ì„œ', 'ë³¸ëŒ€ë¡œ', 'ë¶€ë¥˜ì˜ ì‚¬ëŒë“¤',
    'ë¶ˆêµ¬í•˜ê³ ', 'ì‚¬', 'ì‚¼', 'ìƒëŒ€ì ìœ¼ë¡œ ë§í•˜ìë©´', 'ì„¤ë ¹', 'ì„¤ë§ˆ', 'ì…‹', 'ì†Œìƒ', 'ìˆ˜', 'ìŠµë‹ˆê¹Œ',
    'ìŠµë‹ˆë‹¤', 'ì‹œê°„', 'ì‹œì‘í•˜ì—¬', 'ì‹¤ë¡œ', 'ì‹¬ì§€ì–´', 'ì•„', 'ì•„ë‹ˆ', 'ì•„ë‹ˆë¼ë©´', 'ì•„ë‹ˆë©´', 'ì•„ì•¼',
    'ì•„ìš¸ëŸ¬', 'ì•ˆ ê·¸ëŸ¬ë©´', 'ì•Šê¸° ìœ„í•˜ì—¬', 'ì•Œ ìˆ˜ ìˆë‹¤', 'ì•ì—ì„œ', 'ì•½ê°„', 'ì–‘ì', 'ì–´', 'ì–´ëŠ',
    'ì–´ë””', 'ì–´ë•Œ', 'ì–´ë– í•œ', 'ì–´ë–¤', 'ì–´ë–»ê²Œ', 'ì–´ì§¸ì„œ', 'ì–´ì¨‹ë“ ', 'ì–¸ì œ', 'ì–¼ë§ˆ', 'ì–¼ë§ˆë§Œí¼',
    'ì—‰ì—‰', 'ì—', 'ì—ê²Œ', 'ì—ì„œ', 'ì—ì´', 'ì—”', 'ì˜', 'ì˜ˆ', 'ì˜¬', 'ì™€', 'ì™€ë¥´ë¥´', 'ì™€ì•„', 'ì™œ',
    'ì™¸ì—', 'ìš”', 'ìš°ë¦¬', 'ì›', 'ì›”', 'ìœ„ì—ì„œ ì„œìˆ í•œë°”ì™€ê°™ì´', 'ìœ™ìœ™', 'ìœ¡', 'ìœ¼ë¡œ', 'ìœ¼ë¡œì„œ',
    'ìœ¼ë¡œì¨', 'ì„', 'ì‘', 'ì‘ë‹¹', 'ì˜', 'ì˜ê±°í•˜ì—¬', 'ì˜ì§€í•˜ì—¬', 'ì˜í•´', 'ì´', 'ì´ë¥´ë‹¤', 'ì´ìª½',
    'ì¸ì  ', 'ì¼', 'ì¼ê²ƒì´ë‹¤', 'ì„ì— í‹€ë¦¼ì—†ë‹¤', 'ìê¸°', 'ìê¸°ì§‘', 'ìë§ˆì', 'ìì‹ ', 'ì ê¹', 'ì ì‹œ',
    'ì €', 'ì €ê¸°', 'ì €ìª½', 'ì „ë¶€', 'ì „ì', 'ì •ë„ì— ì´ë¥´ë‹¤', 'ì œ', 'ì œì™¸í•˜ê³ ', 'ì¡°ì°¨', 'ì¢‹ì•„', 'ì¢ì¢',
    'ì£¼', 'ì£¼ì €í•˜ì§€ ì•Šê³ ', 'ì¤„', 'ì¤‘ì´ë‹¤', 'ì¦ˆìŒ', 'ì¦‰', 'ì§€ê¸ˆ', 'ì§€ë§ê³ ', 'ì§„ì§œë¡œ', 'ìª½ìœ¼ë¡œ',
    'ì¯¤', 'ì°¨ë¼ë¦¬', 'ì°¸', 'ì²«ë²ˆì§¸ë¡œ', 'ì³‡', 'ì½¸ì½¸', 'ì¾…ì¾…', 'ì¿µ', 'í¼', 'íƒ€ë‹¤', 'í†µí•˜ë‹¤', 'í‹ˆíƒ€',
    'íŒ', 'í½', 'í•˜', 'í•˜ê²Œë ê²ƒì´ë‹¤', 'í•˜ê²Œí•˜ë‹¤', 'í•˜ê² ëŠ”ê°€', 'í•˜ê³ ', 'í•˜ê³ ìˆì—ˆë‹¤', 'í•˜ê³¤í•˜ì˜€ë‹¤',
    'í•˜êµ¬ë‚˜', 'í•˜ê¸° ë•Œë¬¸ì—', 'í•˜ê¸°ë§Œ í•˜ë©´', 'í•˜ê¸°ë³´ë‹¤ëŠ”', 'í•˜ê¸°ì—', 'í•˜ë‚˜', 'í•˜ëŠë‹ˆ', 'í•˜ëŠ” ê²ƒë§Œ',
    'í•˜ëŠ” í¸ì´ ë‚«ë‹¤', 'í•˜ëŠ”ê²ƒë„', 'í•˜ë”ë¼ë„', 'í•˜ë„ë‹¤', 'í•˜ë„ë¡ì‹œí‚¤ë‹¤', 'í•˜ë„ë¡í•˜ë‹¤', 'í•˜ë“ ì§€',
    'í•˜ë ¤ê³ í•˜ë‹¤', 'í•˜ë§ˆí„°ë©´', 'í•˜ë©´ í• ìˆ˜ë¡', 'í•˜ë©´ì„œ', 'í•˜ë¬¼ë©°', 'í•˜ì—¬ê¸ˆ', 'í•˜ì—¬ì•¼', 'í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´',
    'í•˜ì§€ ì•Šë„ë¡', 'í•˜ì§€ë§ˆ', 'í•˜ì§€ë§ˆë¼', 'í•˜ì²œ', 'í•˜í’ˆ', 'í•œ', 'í•œ ì´ìœ ëŠ”', 'í•œ í›„', 'í•œë‹¤ë©´',
    'í•œë‹¤ë©´ ëª°ë¼ë„', 'í•œë°', 'í•œë§ˆë””', 'í•œì ì´ìˆë‹¤', 'í•œì¼ ìœ¼ë¡œëŠ”', 'í•œí¸', 'í•  ë”°ë¦„ì´ë‹¤', 'í•  ìƒê°ì´ë‹¤',
    'í•  ì¤„ ì•ˆë‹¤', 'í•  ì§€ê²½ì´ë‹¤', 'í• ë•Œ', 'í• ë§Œí•˜ë‹¤', 'í• ë§ì •', 'í• ë¿', 'í•¨ê»˜', 'í•´ë„',
    'í•´ë´ìš”', 'í•´ì„œëŠ”', 'í•´ì•¼í•œë‹¤', 'í•´ìš”', 'í–ˆì–´ìš”', 'í˜•ì‹ìœ¼ë¡œ ì“°ì—¬', 'í˜¹ì‹œ', 'í˜¹ì€', 'í˜¼ì', 'í›¨ì”¬',
    'íœ˜ìµ', 'íœ´', 'íí', 'í˜ì…ì–´',
    'ê°€.', 'ë‚˜.', 'ë‹¤.', 'ë¼.', 'ë§ˆ.', 'ë°”.', 'ì‚¬.', 'ì•„.', 'ì.', 'ì°¨.', 'ì¹´.', 'íƒ€.', 'íŒŒ.', 'í•˜.',
    '1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.', '10.', '11.', '12.', '13.', '14.', '15.',
    'â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤', 'â‘¥', 'â‘¦', 'â‘§', 'â‘¨', 'â‘©',
    'â… ', 'â…¡', 'â…¢', 'â…£', 'â…¤', 'â…¥', 'â…¦', 'â…§', 'â…¨', 'â…©',
    'i.', 'ii.', 'iii.', 'iv.', 'v.', 'vi.', 'vii.', 'viii.', 'ix.', 'x.',
    'ë²•ë¥ ', 'ë²•', 'ì¡°ë¡€', 'ê·œì •', 'ì¡°í•­', 'ì¡°ë¬¸', 'ì¡°ì¹˜', 'ì¡°ì •', 'ê·œì¹™',
    'ë²•ì•ˆ', 'ì…ë²•', 'ê°œì •', 'ì œì •', 'ì‹œí–‰', 'ê³µí¬', 'íì§€', 'ì¼ë¶€ê°œì •', 'ì „ë¶€ê°œì •', 'ë™ì˜ì•ˆ', 'ìŠ¹ì¸ì•ˆ', 'ê²°ì˜ì•ˆ', 'ê±´ì˜ì•ˆ', 'ê·œì¹™ì•ˆ', 'ì„ ì¶œì•ˆ',
    'ë°œì˜', 'ì œì¶œ', 'ì œì•ˆ', 'ì œì˜', 'ì˜ê²°', 'ë¶€ê²°', 'íê¸°', 'ê°€ê²°', 'ì±„íƒ',
    'ì…ë²•ì˜ˆê³ ', 'ì²œë§Œ', 'ê¸°ê´€', 'ê¸°ê°„'
}
preserve_terms = {'ë²•ë¥ ', 'ë²•ì•ˆ', 'ì…ë²•', 'ê°œì •', 'ì œì •', 'ì‹œí–‰', 'ê³µí¬', 'íì§€', 'ì¡°ë¡€', 'ê·œì •', 'ì¡°í•­', 'ì˜ê²°'}
stopwords = {term for term in stopwords if term not in preserve_terms}
excluded_terms = {
    'ì£¼ìš”', 'ìˆ˜ì‚¬', 'ê´€ë ¨', 'ì‚¬í•­', 'ì •ì±…', 'ëŒ€ìƒ', 'ë°©ì•ˆ', 'ì¶”ì§„', 'ê°•í™”', 'ê°œì„ ', 'ì§€ì›',
    'í™•ëŒ€', 'ì¡°ì¹˜', 'í•„ìš”', 'í˜„í™©', 'ê¸°ë°˜', 'ê³¼ì •', 'ê¸°ì¡´', 'ê·¼ê±°', 'ê¸°ëŠ¥', 'ë°©ì‹', 'ë²”ìœ„'
}
excluded_bigrams = {'êµìœ¡ ì‹¤ì‹œ', 'ì§•ì—­ ë²Œê¸ˆ', 'ìˆ˜ë¦½ ì‹œí–‰'}

# ì „ì²˜ë¦¬ í•¨ìˆ˜ (ìµœì í™”)
def preprocess_text(text):
    text = str(text).replace('ï¼Ÿ', '').replace('?', '')
    text = re.sub(r'[^\uAC00-\uD7A3a-zA-Z0-9\s]', ' ', text)
    patterns = [(r'(\w+)\s+(ì—…)', r'\1ì—…')]
    for pattern, replacement in patterns:
        text = re.sub(pattern, replacement, text)
    return re.sub(r'\s+', ' ', text).strip()

def extract_nouns(texts):
    """ë²¡í„°í™”ëœ í˜•íƒœì†Œ ë¶„ì„"""
    results = []
    for text in texts:
        try:
            tokens = kiwi.tokenize(preprocess_text(text))
            words = [
                token.form for token in tokens
                if token.tag.startswith('N') and
                not any(c.isdigit() for c in token.form) and
                token.form not in excluded_terms and
                token.form not in stopwords
            ]
            results.append(' '.join(words))
        except Exception as e:
            print(f"ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            results.append('')
    return results

def remove_single_char_words(texts):
    """ë‹¨ì¼ ë¬¸ì ì œê±° (ë²¡í„°í™”)"""
    return [' '.join(word for word in text.split() if len(word) > 1) for text in texts]

# ë°ì´í„° ì „ì²˜ë¦¬
df['content'] = extract_nouns(df['content'].tolist())
df['content'] = remove_single_char_words(df['content'].tolist())

# LDAìš© ë²¡í„°í™”
vectorizer = CountVectorizer(
    max_df=0.5,
    min_df=10,
    ngram_range=(1, 2),
    max_features=3000,
    token_pattern=r"(?u)\b\w+\b"
)
X = vectorizer.fit_transform(df['content'])

# ì‹¤ë£¨ì—£ ì ìˆ˜ë¡œ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •
def find_optimal_n_topics(X, min_topics=10, max_topics=50, step=5):
    best_n_topics = min_topics
    best_score = -1
    print("\nğŸ” ì‹¤ë£¨ì—£ ì ìˆ˜ë¡œ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê³„ì‚° ì¤‘...")
    for n_topics in tqdm(range(min_topics, max_topics + 1, step)):
        try:
            lda = LatentDirichletAllocation(
                n_components=n_topics,
                max_iter=10,
                learning_method='batch',
                random_state=42,
                n_jobs=1  # ë‹¨ì¼ ìŠ¤ë ˆë“œ ì‚¬ìš©
            )
            topic_dist = lda.fit_transform(X)
            score = silhouette_score(X, topic_dist.argmax(axis=1))
            print(f"n_topics={n_topics}, ì‹¤ë£¨ì—£ ì ìˆ˜: {score:.4f}")
            if score > best_score:
                best_score = score
                best_n_topics = n_topics
        except Exception as e:
            print(f"n_topics={n_topics} ê³„ì‚° ì˜¤ë¥˜: {e}")
            continue
    print(f"âœ… ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜: {best_n_topics} (ì‹¤ë£¨ì—£ ì ìˆ˜: {best_score:.4f})")
    return best_n_topics

# ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ë¡œ LDA í›ˆë ¨
n_topics = find_optimal_n_topics(X, min_topics=10, max_topics=50, step=5)
lda = LatentDirichletAllocation(
    n_components=n_topics,
    max_iter=10,
    learning_method='batch',
    random_state=42,
    n_jobs=1  # ë‹¨ì¼ ìŠ¤ë ˆë“œ ì‚¬ìš©
)
lda.fit(X)
df['topic'] = lda.transform(X).argmax(axis=1)

# í† í”½ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def get_top_words(model, feature_names, n_top_words=12):
    topic_keywords = {}
    for topic_idx, topic in enumerate(model.components_):
        top_indices = topic.argsort()[::-1][:n_top_words]
        keywords = []
        for i in top_indices:
            word = feature_names[i]
            if (word in stopwords or word in excluded_terms or len(word) == 1 or
                word in excluded_bigrams or
                any(part in stopwords or part in excluded_terms for part in word.split())):
                continue
            keywords.append(word)
            if len(keywords) >= 4:
                break
        topic_keywords[topic_idx] = keywords
    return topic_keywords

# Gemini í‚¤ì›Œë“œ ì •ì œ í•¨ìˆ˜
@lru_cache(maxsize=1000)
def refine_keywords_with_gemini_cached(keywords_tuple):
    keywords = list(keywords_tuple)
    prompt = f'''[í•œêµ­ì–´ ì§€ì‹œì‚¬í•­]
ë‹¤ìŒ í‚¤ì›Œë“œ ëª©ë¡ì—ì„œ ë²•ë ¹ ê´€ë ¨ì„±ì„ ê³ ë ¤í•´ ê°€ì¥ ì¤‘ìš”í•œ 4ê°œì˜ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì½¤ë§ˆë¡œ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥:
- í‚¤ì›Œë“œëŠ” ì£¼ì–´ì§„ ë²•ë ¹ ë¬¸ì„œì™€ ì£¼ì œì ìœ¼ë¡œ ì—°ê´€ì„±ì´ ë†’ì•„ì•¼ í•¨
- ì¤‘ë³µ/ìœ ì‚¬ì–´ í†µí•©
- ì¼ë°˜ì  ë‹¨ì–´ ì œì™¸
- ë²•ë ¹ ìš©ì–´ ìš°ì„ ì„ íƒ
- ê²°ê³¼ëŠ” ì •í™•íˆ 4ê°œì˜ í‚¤ì›Œë“œë¡œ, ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ë°˜í™˜
- ë¶ˆìš©ì–´ ë° ì œì™¸ ë‹¨ì–´, ë°”ì´ê·¸ë¨ì€ í¬í•¨ì‹œí‚¤ì§€ ì•ŠìŒ
- ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ë©”íƒ€ ì •ë³´ëŠ” ì ˆëŒ€ í¬í•¨ì‹œí‚¤ì§€ ì•ŠìŒ

ì›ë³¸ í‚¤ì›Œë“œ: {", ".join(keywords)}
'''
    max_retries = 3
    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt)
            if response.text:
                cleaned = response.text.strip().replace('```', '').replace('\n', '')
                if cleaned.startswith('{') or cleaned.startswith('['):
                    try:
                        kw_list = json.loads(cleaned)
                        if isinstance(kw_list, dict):
                            kw_list = kw_list.get('refined_keywords', [])
                        kw_list = [str(kw).strip() for kw in kw_list]
                    except json.JSONDecodeError:
                        kw_list = [kw.strip() for kw in cleaned.strip('[]{}').split(',')]
                else:
                    kw_list = [kw.strip() for kw in cleaned.split(',') if kw.strip()]

                filtered_keywords = []
                for kw in kw_list:
                    if (kw in stopwords or kw in excluded_terms or kw in excluded_bigrams or
                        len(kw) <= 1 or
                        any(part in stopwords or part in excluded_terms for part in kw.split()) or
                        any(kw in existing for existing in filtered_keywords)):
                        continue
                    filtered_keywords.append(kw)
                    if len(filtered_keywords) >= 4:
                        break

                if len(filtered_keywords) >= 4:
                    return ", ".join(filtered_keywords[:4])
                for kw in keywords:
                    if (kw not in filtered_keywords and
                        kw not in stopwords and
                        kw not in excluded_terms and
                        kw not in excluded_bigrams and
                        len(kw) > 1 and
                        not any(part in stopwords or part in excluded_terms for part in kw.split()) and
                        not any(kw in existing for existing in filtered_keywords)):
                        filtered_keywords.append(kw)
                    if len(filtered_keywords) >= 4:
                        break
                if len(filtered_keywords) >= 4:
                    return ", ".join(filtered_keywords[:4])
            time.sleep(1)
        except Exception as e:
            print(f"âš ï¸ í‚¤ì›Œë“œ ì •ì œ ì˜¤ë¥˜ (ì‹œë„ {attempt+1}/{max_retries}): {str(e)[:100]}")
            if attempt < max_retries - 1:
                time.sleep(2)
            continue

    # Fallback: ì›ë³¸ í‚¤ì›Œë“œì—ì„œ 4ê°œ ì„ íƒ
    filtered_keywords = [
        kw for kw in keywords
        if kw not in stopwords and
        kw not in excluded_terms and
        kw not in excluded_bigrams and
        len(kw) > 1 and
        not any(part in stopwords or part in excluded_terms for part in kw.split())
    ]
    if len(filtered_keywords) >= 4:
        return ", ".join(filtered_keywords[:4])
    extra_keywords = [
        feature_names[i] for i in lda.components_[0].argsort()[::-1][12:24]
        if feature_names[i] not in stopwords and
        feature_names[i] not in excluded_terms and
        feature_names[i] not in excluded_bigrams and
        len(feature_names[i]) > 1 and
        not any(part in stopwords or part in excluded_terms for part in feature_names[i].split())
    ]
    filtered_keywords.extend(extra_keywords)
    return ", ".join(filtered_keywords[:4])

def refine_keywords_parallel(topic_keywords):
    topic_labels_refined = {}
    with ThreadPoolExecutor(max_workers=4) as executor:
        future_to_topic = {
            executor.submit(
                refine_keywords_with_gemini_cached,
                tuple(keywords)
            ): topic_id
            for topic_id, keywords in topic_keywords.items()
        }
        for future in tqdm(as_completed(future_to_topic), total=len(future_to_topic)):
            topic_id = future_to_topic[future]
            try:
                refined = future.result()
                topic_labels_refined[topic_id] = refined
            except Exception as e:
                print(f"âš ï¸ í† í”½ {topic_id} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)[:100]}")
                keywords = topic_keywords[topic_id]
                filtered_keywords = [
                    kw for kw in keywords
                    if kw not in stopwords and
                    kw not in excluded_terms and
                    kw not in excluded_bigrams and
                    len(kw) > 1 and
                    not any(part in stopwords or part in excluded_terms for part in kw.split())
                ]
                if len(filtered_keywords) < 4:
                    extra_keywords = [
                        feature_names[i] for i in lda.components_[topic_id].argsort()[::-1][12:24]
                        if feature_names[i] not in stopwords and
                        feature_names[i] not in excluded_terms and
                        feature_names[i] not in excluded_bigrams and
                        len(feature_names[i]) > 1 and
                        not any(part in stopwords or part in excluded_terms for part in feature_names[i].split())
                    ]
                    filtered_keywords.extend(extra_keywords[:4-len(filtered_keywords)])
                topic_labels_refined[topic_id] = ", ".join(filtered_keywords[:4])
    return topic_labels_refined

# í† í”½ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì •ì œ
feature_names = np.array(vectorizer.get_feature_names_out())
topic_keywords = get_top_words(lda, feature_names, n_top_words=12)

print("\nğŸ”§ Gemini APIë¥¼ ì´ìš©í•œ í‚¤ì›Œë“œ ì •ì œ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)...")
topic_labels_refined = refine_keywords_parallel(topic_keywords)

# ëª¨ë“  í† í”½ì— í‚¤ì›Œë“œ í• ë‹¹
for topic_id in range(n_topics):
    if topic_id not in topic_labels_refined:
        topic = lda.components_[topic_id]
        keywords = [
            feature_names[i] for i in topic.argsort()[::-1][:12]
            if feature_names[i] not in stopwords and
            feature_names[i] not in excluded_terms and
            feature_names[i] not in excluded_bigrams and
            len(feature_names[i]) > 1 and
            not any(part in stopwords or part in excluded_terms for part in feature_names[i].split())
        ]
        topic_labels_refined[topic_id] = ", ".join(keywords[:4])

# ê²°ê³¼ ë§¤í•‘
df['topic_label'] = df['topic'].map(topic_labels_refined)
df['topic_label'] = df['topic_label'].fillna(", ".join(feature_names[lda.components_[0].argsort()[::-1][:4]]))

# ìµœì¢… ì €ì¥
output_path = settings.BASE_DIR / 'keword_clustering' / 'data' / 'bill_keyword_clustering_refined.csv'
df[['age', 'title', 'bill_id', 'bill_number', 'content', 'topic', 'topic_label']].to_csv(
    output_path,
    index=False,
    quoting=csv.QUOTE_NONNUMERIC,
    encoding='utf-8-sig'
)

print("âœ… ì²˜ë¦¬ ì™„ë£Œ! íŒŒì¼: bill_keyword_clustering_refined.csv")