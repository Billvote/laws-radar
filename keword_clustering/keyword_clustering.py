# -*- coding: utf-8 -*-
import sys
import gc
import pickle
import asyncio
import aiohttp
from pathlib import Path
import pandas as pd
from tqdm import tqdm
import numpy as np
import google.generativeai as genai
import time
import json
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import Counter, defaultdict

tqdm.pandas()

# ê¸°ì¡´ í•„í„° ì„¤ì • ìœ ì§€ + ë²•ë¥  íŠ¹í™” ìš©ì–´ ì¶”ê°€
custom_nouns = [
    'ëŒ€í†µë ¹ë¹„ì„œì‹¤', 'êµ­ê°€ì•ˆë³´ì‹¤', 'ëŒ€í†µë ¹ê²½í˜¸ì²˜', 'í—Œë²•ìƒëŒ€í†µë ¹ìë¬¸ê¸°êµ¬', 'êµ­ê°€ì•ˆì „ë³´ì¥íšŒì˜',
    'ë¯¼ì£¼í‰í™”í†µì¼ìë¬¸íšŒì˜', 'êµ­ë¯¼ê²½ì œìë¬¸íšŒì˜', 'êµ­ê°€ê³¼í•™ê¸°ìˆ ìë¬¸íšŒì˜', 'ê°ì‚¬ì›', 'êµ­ê°€ì •ë³´ì›',
    'ë°©ì†¡í†µì‹ ìœ„ì›íšŒ', 'íŠ¹ë³„ê°ì°°ê´€', 'ê³ ìœ„ê³µì§ìë²”ì£„ìˆ˜ì‚¬ì²˜', 'êµ­ê°€ì¸ê¶Œìœ„ì›íšŒ', 'êµ­ë¬´ì¡°ì •ì‹¤',
    'êµ­ë¬´ì´ë¦¬ë¹„ì„œì‹¤', 'ì¸ì‚¬í˜ì‹ ì²˜', 'ë²•ì œì²˜', 'ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜', 'ê³µì •ê±°ë˜ìœ„ì›íšŒ',
    'êµ­ë¯¼ê¶Œìµìœ„ì›íšŒ', 'ê¸ˆìœµìœ„ì›íšŒ', 'ê°œì¸ì •ë³´ë³´í˜¸ìœ„ì›íšŒ', 'ì›ìë ¥ì•ˆì „ìœ„ì›íšŒ', 'ê¸°íšì¬ì •ë¶€',
    'êµ­ì„¸ì²­', 'ê´€ì„¸ì²­', 'ì¡°ë‹¬ì²­', 'í†µê³„ì²­', 'êµìœ¡ë¶€', 'ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€', 'ìš°ì£¼í•­ê³µì²­',
    'ì™¸êµë¶€', 'ì¬ì™¸ë™í¬ì²­', 'í†µì¼ë¶€', 'ë²•ë¬´ë¶€', 'ê²€ì°°ì²­', 'êµ­ë°©ë¶€', 'ë³‘ë¬´ì²­', 'ë°©ìœ„ì‚¬ì—…ì²­',
    'í–‰ì •ì•ˆì „ë¶€', 'ê²½ì°°ì²­', 'ì†Œë°©ì²­', 'êµ­ê°€ë³´í›ˆë¶€', 'ë¬¸í™”ì²´ìœ¡ê´€ê´‘ë¶€', 'êµ­ê°€ìœ ì‚°ì²­',
    'ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€', 'ë†ì´Œì§„í¥ì²­', 'ì‚°ë¦¼ì²­', 'ì‚°ì—…í†µìƒìì›ë¶€', 'íŠ¹í—ˆì²­', 'ë³´ê±´ë³µì§€ë¶€',
    'ì§ˆë³‘ê´€ë¦¬ì²­', 'í™˜ê²½ë¶€', 'ê¸°ìƒì²­', 'ê³ ìš©ë…¸ë™ë¶€', 'ì—¬ì„±ê°€ì¡±ë¶€', 'êµ­í† êµí†µë¶€',
    'í–‰ì •ì¤‘ì‹¬ë³µí•©ë„ì‹œê±´ì„¤ì²­', 'ìƒˆë§Œê¸ˆê°œë°œì²­', 'í•´ì–‘ìˆ˜ì‚°ë¶€', 'í•´ì–‘ê²½ì°°ì²­', 'ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€',
    'ìƒì„ìœ„ì›íšŒ', 'ë²•ì œì‚¬ë²•ìœ„ì›íšŒ', 'ì •ë¬´ìœ„ì›íšŒ', 'ê¸°íšì¬ì •ìœ„ì›íšŒ', 'êµìœ¡ìœ„ì›íšŒ',
    'ê³¼í•™ê¸°ìˆ ì •ë³´ë°©ì†¡í†µì‹ ìœ„ì›íšŒ', 'ì™¸êµí†µì¼ìœ„ì›íšŒ', 'êµ­ë°©ìœ„ì›íšŒ', 'í–‰ì •ì•ˆì „ìœ„ì›íšŒ',
    'ë¬¸í™”ì²´ìœ¡ê´€ê´‘ìœ„ì›íšŒ', 'ë†ë¦¼ì¶•ì‚°ì‹í’ˆí•´ì–‘ìˆ˜ì‚°ìœ„ì›íšŒ', 'ì‚°ì—…í†µìƒìì›ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ìœ„ì›íšŒ',
    'ë³´ê±´ë³µì§€ìœ„ì›íšŒ', 'í™˜ê²½ë…¸ë™ìœ„ì›íšŒ', 'êµ­í† êµí†µìœ„ì›íšŒ', 'ì •ë³´ìœ„ì›íšŒ', 'ì—¬ì„±ê°€ì¡±ìœ„ì›íšŒ',
    'ì˜ˆì‚°ê²°ì‚°íŠ¹ë³„ìœ„ì›íšŒ', 'íŠ¹ë³„ìœ„ì›íšŒ', 'ì†Œìœ„ì›íšŒ', 'ë²•ì•ˆì‹¬ì‚¬ì†Œìœ„', 'ì˜ì•ˆ', 'ë²•ë¥ ì•ˆ',
    'ì˜ˆì‚°ì•ˆ', 'ë™ì˜ì•ˆ', 'ìŠ¹ì¸ì•ˆ', 'ê²°ì˜ì•ˆ', 'ê±´ì˜ì•ˆ', 'ê·œì¹™ì•ˆ', 'ì„ ì¶œì•ˆ', 'ë°œì˜', 'ì œì¶œ',
    'ì œì•ˆ', 'ì œì˜', 'ì˜ê²°', 'ë¶€ê²°', 'íê¸°', 'ê°€ê²°', 'ì±„íƒ', 'ì…ë²•ì˜ˆê³ ', 'ê³µí¬', 'ì‹œí–‰',
    'ê°œì •', 'ì œì •', 'íì§€', 'ì¼ë¶€ê°œì •', 'ì „ë¶€ê°œì •',
    # ë²•ë¥  íŠ¹í™” ìš©ì–´ ì¶”ê°€
    'ì†¡í™˜ëŒ€ê¸°ì‹¤', 'ì…êµ­ë¶ˆí—ˆ', 'ë°€ì…êµ­', 'ì¶œì…êµ­ê´€ë¦¬', 'ì™¸êµ­ì¸ê´€ì„œ', 'ìš´ìˆ˜ì—…ì', 'í•­ê³µì‚¬ìš´ì˜í˜‘ì˜íšŒ',
    'êµ­ë¯¼ì•ˆì „', 'ìœ„í˜‘ìš”ì†Œ', 'ì‚¬í›„ê´€ë¦¬', 'ë³´ì•ˆì•ˆì „', 'ê°ì‚¬ê²°ê³¼', 'ë²•ì ê·¼ê±°', 'íŠ¹ë³„ì‚¬ìœ '
]

initial_stopwords = frozenset({
    'ì¡°', 'í•­', 'í˜¸', 'ê²½ìš°', 'ë“±', 'ìˆ˜', 'ê²ƒ', 'ì´', 'ì°¨', 'í›„', 'ì´ìƒ', 'ì´í•˜', 'ì´ë‚´',
    'ì•ˆ', 'ì†Œ', 'ëŒ€', 'ì ', 'ê°„', 'ê³³', 'í•´ë‹¹', 'ì™¸', 'ë‚˜', 'ë°”', 'ì‹œ', 'ê´€ë ¨', 'ê´€í•˜ì—¬',
    'ëŒ€í•˜ì—¬', 'ë”°ë¼', 'ë”°ë¥¸', 'ìœ„í•˜ì—¬', 'ì˜í•˜ì—¬', 'ë•Œ', 'ê°', 'ì', 'ì¸', 'ë‚´', 'ì¤‘',
    'ë•Œë¬¸', 'ìœ„í•´', 'í†µí•´', 'ë¶€í„°', 'ê¹Œì§€', 'ë™ì•ˆ', 'ì‚¬ì´', 'ê¸°ì¤€', 'ë³„ë„', 'ë³„ì²¨', 'ë³„í‘œ',
    'ì œí•œ', 'íŠ¹ì¹™', 'ê°€ëŠ¥', 'ê³¼ì •', 'ê¸°ë°˜', 'ê¸°ì¡´', 'ê·¼ê±°', 'ê¸°ëŠ¥', 'ë°©ì‹', 'ë²”ìœ„', 'ì‚¬í•­',
    'ì‹œì ', 'ìµœê·¼', 'ë…„', 'ì¥', 'í•´', 'ëª…', 'ë‚ ', 'íšŒ', 'ë™', 'ë°', 'êµ­', 'ë°–', 'ì†', 'ì‹',
    'ê·œ', 'í˜„í–‰ë²•', 'ì§', 'ë²”', 'ë§Œ', 'ì…', 'ì‹ ',
})

initial_excluded_terms = frozenset({
    'ì£¼ìš”', 'ìˆ˜ì‚¬', 'ê´€ë ¨', 'ì‚¬í•­', 'ì •ì±…', 'ëŒ€ìƒ', 'ë°©ì•ˆ', 'ì¶”ì§„', 'ê°•í™”', 'ê°œì„ ', 'ì§€ì›',
    'í™•ëŒ€', 'ì¡°ì¹˜', 'í•„ìš”', 'í˜„í™©', 'ê¸°ë°˜', 'ê³¼ì •', 'ê¸°ì¡´', 'ê·¼ê±°', 'ê¸°ëŠ¥', 'ë°©ì‹', 'ë²”ìœ„',
    'í™œë™', 'ìš´ì˜', 'ê´€ë¦¬', 'ì‹¤ì‹œ', 'í™•ë³´', 'êµ¬ì„±', 'ì„¤ì¹˜', 'ì§€ì •', 'ê³„íš', 'ìˆ˜ë¦½',
})

# ë²•ë¥  íŠ¹í™” ë¶ˆìš©ì–´ ì¶”ê°€
legal_specific_stopwords = frozenset({
    'ìˆëŠ”', 'ìˆìŒ', 'ë˜ëŠ”', 'ë˜ë„ë¡', 'í•˜ëŠ”', 'í•˜ë„ë¡', 'ì§€ì í•œ', 'ë§ˆë ¨í• ', 'ë¶€ì—¬í•¨ì—',
    'ìƒí™©ì„', 'ìˆê²Œ', 'í•¨ìœ¼ë¡œì¨', 'í•˜ê³ ì', 'ê²½ìš°ì—ëŠ”', 'ìˆëŠ”ì§€', 'ìˆë‹¤ë©´', 'í•˜ì—¬ì•¼',
    'í•˜ì—¬ì„œëŠ”', 'ì•„ë‹ˆí•˜ëŠ”', 'ì•„ë‹ˆí•œ', 'ì•„ë‹ˆë˜ëŠ”', 'ì•„ë‹ˆëœ', 'ìˆìœ¼ë¯€ë¡œ', 'ìˆì–´ì„œ',
    'ê°€ëŠ¥í•œ', 'í•„ìš”í•œ', 'ì ì ˆí•œ', 'íš¨ê³¼ì ì¸', 'ì§€ì†ì ìœ¼ë¡œ', 'ì „ë¬¸ì ì¸', 'ì²´ê³„ì ì¸'
})

preserve_terms = frozenset({
    'ë²•ë¥ ', 'ë²•ì•ˆ', 'ì…ë²•', 'ê°œì •', 'ì œì •', 'ì‹œí–‰', 'ê³µí¬', 'íì§€', 'ì¡°ë¡€', 'ê·œì •', 'ì¡°í•­', 'ì˜ê²°',
    'ê°ì‚¬ì›', 'êµ­ë¯¼', 'ì•ˆì „', 'ìœ„í˜‘', 'ìš”ì†Œ', 'ëŒ€ì‘', 'ì‹¤íƒœ', 'ê³µí•­', 'ë³´ì•ˆ', 'ë¶„ì•¼', 
    'ê°ì‚¬', 'ê²°ê³¼', 'ì…êµ­', 'ë¶ˆí—ˆ', 'ì‚¬í›„', 'ë¯¸í¡', 'ììœ ', 'ì´ë™', 'ì¼ë¶€', 'ë°€ì…êµ­',
    'ì‹œë„', 'ë°œìƒ', 'ì¼ë°˜ì¸', 'ë¶„ë¦¬', 'êµ¬ë³„', 'ì¶œêµ­', 'ì†¡í™˜', 'ëŒ€ê¸°ì‹¤', 'í†µì œ', 'ë§ˆë ¨',
    'ì§€ì ', 'í—ˆê°€', 'ì™¸êµ­ì¸', 'ì„ ë°•', 'ìš´ìˆ˜ì—…', 'ì˜ë¬´', 'ë¶€ì—¬', 'ë¯¼ê°„', 'í•­ê³µì‚¬',
    'í˜‘ì˜íšŒ', 'ë³¸êµ­', 'ì„ì‹œ', 'ë¬¸ì œì ', 'ì œê¸°', 'ì§€ë°©', 'ê´€ì„œ', 'íš¨ê³¼', 'ì¼ì •', 'ì¥ì†Œ',
    'ì œê³µ', 'ìš”ì²­', 'íŠ¹ë³„', 'ì‚¬ìœ ', 'í˜‘ì¡°', 'ì‹ ì„¤'
})

excluded_bigrams = frozenset({'êµìœ¡ ì‹¤ì‹œ', 'ì§•ì—­ ë²Œê¸ˆ', 'ìˆ˜ë¦½ ì‹œí–‰', 'ìš´ì˜ ê´€ë¦¬'})

# Gemini API í‚¤ ì„¤ì •
GEMINI_API_KEY = "AIzaSyA8M00iSzCK1Lvc5YfxamYgQf-Lh4xh5R0"
genai.configure(api_key=GEMINI_API_KEY)

# ===== ë²•ë¥  ë¬¸ì„œ íŠ¹í™” ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ =====

def enhanced_law_pattern_removal(text):
    """í–¥ìƒëœ ë²•ë¥  êµ¬ì¡° íŒ¨í„´ ì œê±°"""
    if pd.isna(text) or text is None or not isinstance(text, str):
        return ""
    
    try:
        patterns = [
            # ê¸°ë³¸ ë²•ì¡°ë¬¸ íŒ¨í„´
            r'ì œ?\d+ì¡°ì˜?\d*(?:ì œ?\d+í•­)?(?:ì œ?\d+í˜¸)?',
            r'ì•ˆ\s*ì œ?\d+ì¡°ì˜?\d*(?:ì œ?\d+í•­)?(?:ì œ?\d+í˜¸)?',
            
            # ë‚ ì§œ ë° ìˆ˜ëŸ‰ íŒ¨í„´
            r'\d+ë…„\s*\d+ì›”\s*\d+ì¼?',
            r'\d+ë§Œ\s*\d+ì²œ?\s*\d+ëª…?',
            r'\d+%',
            
            # ë²•ë¥  ë¬¸ì„œ íŠ¹ìˆ˜ íŒ¨í„´
            r'\([^)]*ë²•[^)]*\)',  # ë²•ë¥ ëª… ê´„í˜¸
            r'\([^)]*ë…„[^)]*\)',  # ë…„ë„ ê´„í˜¸
            r'\'[^\']*\'',        # ì‘ì€ë”°ì˜´í‘œ ë‚´ìš©
            r'"[^"]*"',           # í°ë”°ì˜´í‘œ ë‚´ìš©
            r'ï¼Ÿ',                # íŠ¹ìˆ˜ ë¬¼ìŒí‘œ
            
            # ë¶ˆí•„ìš”í•œ ì¡°ì‚¬ ë° ì–´ë¯¸
            r'\b(?:ëˆ„êµ¬ë‚˜|ì§€ë‹ˆê³ |ìœ ì‚¬í•œ|ê¸°ì¤€|ì•½)\b',
            r'ì‹ ì„¤', r'ì •ë¹„', r'ì¡°ì •', r'ì¸ìš©ì¡°ë¬¸', r'ì •ë¹„\s*\(.*?\)', r'ì•ˆ'
        ]
        
        combined = '|'.join(patterns)
        text = re.sub(combined, ' ', text)
        return re.sub(r'\s+', ' ', text).strip()
        
    except Exception as e:
        print(f"ë²•ë ¹ êµ¬ì¡° íŒ¨í„´ ì œê±° ì˜¤ë¥˜: {str(e)}")
        return text

def compound_noun_handler(text):
    """ë³µí•© ëª…ì‚¬ ë° íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬"""
    if not text:
        return ""
    
    try:
        # 1. íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬
        text = re.sub(r'Â·', ' ', text)  # ì¤‘ì  ì œê±°
        text = re.sub(r'ï¼Ÿ', ' ', text)  # íŠ¹ìˆ˜ ë¬¼ìŒí‘œ ì œê±°
        
        # 2. ë³µí•© ëª…ì‚¬ ë¶„ë¦¬ íŒ¨í„´
        compound_patterns = {
            r'ì§€ë°©ì¶œì…êµ­Â·?ì™¸êµ­ì¸ê´€ì„œ': 'ì§€ë°© ì¶œì…êµ­ ì™¸êµ­ì¸ ê´€ì„œ',
            r'í•­ê³µì‚¬ìš´ì˜í˜‘ì˜íšŒ': 'í•­ê³µì‚¬ ìš´ì˜ í˜‘ì˜íšŒ',
            r'ì†¡í™˜ëŒ€ê¸°ì‹¤': 'ì†¡í™˜ ëŒ€ê¸°ì‹¤',
            r'ì…êµ­ë¶ˆí—ˆì': 'ì…êµ­ ë¶ˆí—ˆ',
            r'ë°€ì…êµ­ì‹œë„': 'ë°€ì…êµ­ ì‹œë„',
            r'ì‚¬í›„ê´€ë¦¬': 'ì‚¬í›„ ê´€ë¦¬',
            r'ë³´ì•ˆì•ˆì „': 'ë³´ì•ˆ ì•ˆì „',
            r'ë²•ì ê·¼ê±°': 'ë²•ì  ê·¼ê±°',
            r'ìœ„í˜‘ìš”ì†Œ': 'ìœ„í˜‘ ìš”ì†Œ'
        }
        
        for pattern, replacement in compound_patterns.items():
            text = re.sub(pattern, replacement, text)
        
        # 3. ì—°ì†ëœ ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
        
    except Exception as e:
        print(f"ë³µí•© ëª…ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        return text

def context_aware_stopword_filter(text):
    """ë§¥ë½ ì¸ì‹ ê³„ì¸µì  ë¶ˆìš©ì–´ ì²˜ë¦¬"""
    if not text:
        return ""
    
    try:
        words = text.split()
        filtered_words = []
        
        for i, word in enumerate(words):
            # ê¸¸ì´ í•„í„°ë§
            if len(word) < 2:
                continue
            
            # ìˆ«ìë§Œìœ¼ë¡œ êµ¬ì„±ëœ ë‹¨ì–´ ì œê±°
            if word.isdigit():
                continue
            
            # ê¸°ë³¸ ë¶ˆìš©ì–´ ì²´í¬
            if word in initial_stopwords:
                continue
            
            # ì œì™¸ ìš©ì–´ ì²´í¬
            if word in initial_excluded_terms:
                continue
            
            # ë²•ë¥  íŠ¹í™” ë¶ˆìš©ì–´ ì²´í¬
            if word in legal_specific_stopwords:
                continue
            
            # ë³´ì¡´ ìš©ì–´ëŠ” í•­ìƒ ìœ ì§€
            if word in preserve_terms:
                filtered_words.append(word)
                continue
            
            # ì‚¬ìš©ì ì •ì˜ ë²•ë¥  ìš©ì–´ ìœ ì§€
            if word in custom_nouns:
                filtered_words.append(word)
                continue
            
            # 2ê¸€ì ì´ìƒ ì¼ë°˜ ìš©ì–´
            if len(word) >= 2:
                # ë™ì‚¬/í˜•ìš©ì‚¬ ì–´ë¯¸ íŒ¨í„´ ì œê±°
                if not re.search(r'(í•˜ë‹¤|ë˜ë‹¤|ìˆë‹¤|ì—†ë‹¤|ì´ë‹¤|ì•„ë‹ˆë‹¤)$', word):
                    filtered_words.append(word)
        
        return ' '.join(filtered_words)
        
    except Exception as e:
        print(f"ë¶ˆìš©ì–´ í•„í„°ë§ ì˜¤ë¥˜: {str(e)}")
        return text

def legal_document_preprocessing_pipeline(text):
    """ë²•ë¥  ë¬¸ì„œ ì „ìš© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    if pd.isna(text) or text is None:
        return ""
    
    if not isinstance(text, str):
        text = str(text)
    
    if not text.strip():
        return ""
    
    try:
        # 1ë‹¨ê³„: ê¸°ë³¸ ë²•ë ¹ êµ¬ì¡° íŒ¨í„´ ì œê±°
        text = enhanced_law_pattern_removal(text)
        
        # 2ë‹¨ê³„: ë³µí•© ëª…ì‚¬ ë° íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬
        text = compound_noun_handler(text)
        
        # 3ë‹¨ê³„: ë§¥ë½ ì¸ì‹ ë¶ˆìš©ì–´ í•„í„°ë§
        text = context_aware_stopword_filter(text)
        
        # 4ë‹¨ê³„: ìµœì¢… ì •ê·œí™”
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text if text else ""
        
    except Exception as e:
        print(f"ë²•ë¥  ë¬¸ì„œ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        return text

def improved_content_preprocessing(df):
    """ê°œì„ ëœ content ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    print("ğŸ“Š ë²•ë¥  ë¬¸ì„œ íŠ¹í™” ì „ì²˜ë¦¬ ì‹œì‘...")
    
    # 1. ë°ì´í„° íƒ€ì… ë¶„í¬ í™•ì¸
    print("ë°ì´í„° íƒ€ì… ë¶„í¬:")
    print(df['content'].apply(lambda x: type(x)).value_counts())
    
    # 2. ê²°ì¸¡ê°’ ì²˜ë¦¬
    print("ğŸ”„ ê²°ì¸¡ê°’ ì²˜ë¦¬ ì¤‘...")
    df['content'] = df['content'].fillna('')
    
    # 3. ë°ì´í„° íƒ€ì… í†µì¼
    print("ğŸ”„ ë°ì´í„° íƒ€ì… í†µì¼ ì¤‘...")
    df['content'] = df['content'].astype(str)
    
    # 4. ë¹ˆ ê°’ ì •ë¦¬
    print("ğŸ”„ ë¹ˆ ê°’ ì •ë¦¬ ì¤‘...")
    df['content'] = df['content'].replace(['nan', 'None'], '')
    
    # 5. ë²•ë¥  ë¬¸ì„œ íŠ¹í™” ì „ì²˜ë¦¬ ì ìš©
    print("ğŸ”„ ë²•ë¥  ë¬¸ì„œ íŠ¹í™” ì „ì²˜ë¦¬ ì ìš© ì¤‘...")
    df['content'] = df['content'].apply(legal_document_preprocessing_pipeline)
    
    # 6. ê²°ê³¼ ê²€ì¦
    empty_count = (df['content'] == '').sum()
    print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df)}ê°œ ì¤‘ {empty_count}ê°œ ë¹ˆ í…ìŠ¤íŠ¸")
    
    # 7. ìƒ˜í”Œ ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“‹ ë²•ë¥  íŠ¹í™” ì „ì²˜ë¦¬ ê²°ê³¼ ìƒ˜í”Œ:")
    for i in range(min(3, len(df))):
        if df.iloc[i]['content']:
            print(f"   {i+1}. {df.iloc[i]['content'][:150]}...")
    
    return df

# ===== 2ë‹¨ê³„: Gemini ì›ë³¸ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ =====

def gemini_clustering_from_original(original_texts, titles, model):
    """Geminiê°€ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ë¶„ì„í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰"""
    print(f"ğŸ¤– Gemini ì›ë³¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘...")
    
    # ë¬¸ì„œë³„ ì£¼ì œ ë¶„ë¥˜
    def classify_document(idx_doc):
        idx, doc, title = idx_doc
        prompt = f"""
ë‹¤ìŒ ë²•ì•ˆì˜ ì›ë³¸ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì£¼ì œ ì¹´í…Œê³ ë¦¬ë¥¼ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.

ë²•ì•ˆ ì œëª©: {title}
ë²•ì•ˆ ì›ë³¸ ë‚´ìš©: {doc[:3000]}

ë‹¤ìŒ ì£¼ì œ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ê±°ë‚˜ ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬ë¥¼ ì œì•ˆí•˜ì„¸ìš”:
- êµìœ¡ì •ì±…
- ë³´ê±´ì˜ë£Œ  
- ê²½ì œê¸ˆìœµ
- í™˜ê²½ì—ë„ˆì§€
- ì‚¬íšŒë³µì§€
- êµ­ë°©ì•ˆë³´
- ë²•ë¬´ì‚¬ë²•
- í–‰ì •ì•ˆì „
- ê³¼í•™ê¸°ìˆ 
- ë¬¸í™”ì²´ìœ¡
- ë†ë¦¼ìˆ˜ì‚°
- êµ­í† êµí†µ
- ì™¸êµí†µì¼
- ë””ì§€í„¸ì •ë³´í†µì‹ 
- ì¶œì…êµ­ê´€ë¦¬
- ê¸°íƒ€

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "category": "ì£¼ì œ_ì¹´í…Œê³ ë¦¬",
  "subcategory": "ì„¸ë¶€_ë¶„ë¥˜",
  "confidence": 0.9
}}
"""
        
        try:
            response = model.generate_content(prompt)
            result_text = response.text.strip()
            
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return idx, result
        except Exception as e:
            print(f"ë¬¸ì„œ {idx} ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        
        return idx, {"category": "ê¸°íƒ€", "subcategory": "ì¼ë°˜", "confidence": 0.5}
    
    # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¬¸ì„œ ë¶„ë¥˜
    document_classifications = {}
    data_with_index = [(i, original_texts[i], titles[i]) for i in range(len(original_texts))]
    
    with ThreadPoolExecutor(max_workers=6) as executor:
        futures = [executor.submit(classify_document, item) for item in data_with_index]
        
        for future in tqdm(as_completed(futures), total=len(futures), desc="ì›ë³¸ í…ìŠ¤íŠ¸ ë¬¸ì„œ ë¶„ë¥˜"):
            idx, classification = future.result()
            document_classifications[idx] = classification
    
    # ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹í™”
    category_groups = defaultdict(list)
    for idx, classification in document_classifications.items():
        category = classification["category"]
        subcategory = classification.get("subcategory", "ì¼ë°˜")
        key = f"{category}_{subcategory}"
        category_groups[key].append(idx)
    
    # í´ëŸ¬ìŠ¤í„° ID í• ë‹¹
    final_clusters = {}
    cluster_id = 0
    
    for category_key, doc_indices in category_groups.items():
        if len(doc_indices) <= 5:
            # ì†Œê·œëª¨ ê·¸ë£¹ì€ ë‹¨ì¼ í´ëŸ¬ìŠ¤í„°
            for doc_idx in doc_indices:
                final_clusters[doc_idx] = cluster_id
            cluster_id += 1
        else:
            # ëŒ€ê·œëª¨ ê·¸ë£¹ì€ ì¶”ê°€ ì„¸ë¶„í™”
            subclusters = gemini_subcluster_documents(
                [(idx, original_texts[idx], titles[idx]) for idx in doc_indices], 
                model, 
                category_key
            )
            
            for subcluster_docs in subclusters:
                for doc_idx in subcluster_docs:
                    final_clusters[doc_idx] = cluster_id
                cluster_id += 1
    
    return final_clusters

def gemini_subcluster_documents(docs_data, model, category):
    """ì¹´í…Œê³ ë¦¬ ë‚´ ì„¸ë¶€ í´ëŸ¬ìŠ¤í„°ë§"""
    if len(docs_data) <= 10:
        return [[doc[0] for doc in docs_data]]
    
    prompt = f"""
ë‹¤ìŒ {category} ì¹´í…Œê³ ë¦¬ì˜ ë²•ì•ˆë“¤ì„ ìœ ì‚¬í•œ ì„¸ë¶€ ì£¼ì œë³„ë¡œ 2-4ê°œ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì£¼ì„¸ìš”.

ë²•ì•ˆ ëª©ë¡:
"""
    
    for i, (idx, text, title) in enumerate(docs_data[:15]):
        prompt += f"{i+1}. {title[:80]}...\n"
    
    prompt += """
ê° ê·¸ë£¹ì— ì†í•˜ëŠ” ë²•ì•ˆ ë²ˆí˜¸ë“¤ì„ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

{
  "groups": [
    {
      "name": "ê·¸ë£¹ëª…1",
      "bills": [1, 3, 5]
    },
    {
      "name": "ê·¸ë£¹ëª…2", 
      "bills": [2, 4, 6]
    }
  ]
}
"""
    
    try:
        response = model.generate_content(prompt)
        result_text = response.text.strip()
        
        json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group())
            
            subclusters = []
            used_indices = set()
            
            for group in result.get("groups", []):
                group_doc_indices = []
                for bill_num in group.get("bills", []):
                    if 1 <= bill_num <= len(docs_data) and (bill_num - 1) not in used_indices:
                        doc_idx = docs_data[bill_num - 1][0]
                        group_doc_indices.append(doc_idx)
                        used_indices.add(bill_num - 1)
                
                if group_doc_indices:
                    subclusters.append(group_doc_indices)
            
            # ë¯¸ë¶„ë¥˜ ë¬¸ì„œë“¤ ì²˜ë¦¬
            remaining = [docs_data[i][0] for i in range(len(docs_data)) if i not in used_indices]
            if remaining:
                subclusters.append(remaining)
            
            return subclusters if subclusters else [[doc[0] for doc in docs_data]]
            
    except Exception as e:
        print(f"ì„¸ë¶€ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}")
    
    return [[doc[0] for doc in docs_data]]

# ===== 3ë‹¨ê³„: Gemini ì›ë³¸ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ =====

def gemini_extract_cluster_keywords(cluster_docs, original_texts, titles, model):
    """í´ëŸ¬ìŠ¤í„°ë³„ ì›ë³¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    # í´ëŸ¬ìŠ¤í„° ë‚´ ëŒ€í‘œ ë¬¸ì„œë“¤ ì„ íƒ
    sample_indices = cluster_docs[:3]  # ìµœëŒ€ 3ê°œ ë¬¸ì„œ
    combined_text = ' '.join([original_texts[i] for i in sample_indices])
    representative_title = titles[sample_indices[0]] if sample_indices else "ë²•ì•ˆ"
    
    prompt = f"""
ë‹¤ìŒ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ë²•ì•ˆë“¤ì˜ ì›ë³¸ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ê³µí†µëœ í•µì‹¬ í‚¤ì›Œë“œ 4ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ëŒ€í‘œ ì œëª©: {representative_title}
í´ëŸ¬ìŠ¤í„° ì›ë³¸ ë‚´ìš©: {combined_text[:4000]}

ì¶”ì¶œ ê·œì¹™:
1. í´ëŸ¬ìŠ¤í„° ë‚´ ëª¨ë“  ë¬¸ì„œì— ê³µí†µìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ìš©ì–´ ìš°ì„ 
2. ë²•ë¥  ì „ë¬¸ìš©ì–´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì„ íƒ
3. ì›ë³¸ ë‚´ìš©ì— ì‹¤ì œë¡œ ë“±ì¥í•˜ëŠ” ìš©ì–´ë§Œ ì‚¬ìš©
4. í´ëŸ¬ìŠ¤í„°ì˜ ì£¼ì œë¥¼ ê°€ì¥ ì˜ ëŒ€í‘œí•˜ëŠ” í‚¤ì›Œë“œ ì„ íƒ

ë°˜ë“œì‹œ ë‹¤ìŒ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3", "í‚¤ì›Œë“œ4"]
"""
    
    try:
        response = model.generate_content(prompt)
        result_text = response.text.strip()
        
        match = re.search(r'\["[^"]*"(?:\s*,\s*"[^"]*")*\]', result_text)
        if match:
            keywords = json.loads(match.group())
            return keywords[:4]
    except Exception as e:
        print(f"í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
    
    return ["ë²•ì•ˆ", "ê°œì •", "ì •ì±…", "ì‹œí–‰"]

def gemini_extract_single_keywords(original_text, title, model):
    """ë‹¨ì¼ ë¬¸ì„œ ì›ë³¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    prompt = f"""
ë‹¤ìŒ ë²•ì•ˆì˜ ì›ë³¸ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ í‚¤ì›Œë“œ 4ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ë²•ì•ˆ ì œëª©: {title}
ë²•ì•ˆ ì›ë³¸ ë‚´ìš©: {original_text[:3000]}

ì¶”ì¶œ ê·œì¹™:
1. ë°˜ë“œì‹œ ì›ë³¸ ë‚´ìš©ì— ì‹¤ì œë¡œ ë“±ì¥í•˜ëŠ” ìš©ì–´ë§Œ ì‚¬ìš©
2. ë²•ë¥  ì „ë¬¸ìš©ì–´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì„ íƒ
3. ë²•ì•ˆì˜ í•µì‹¬ ëª©ì ê³¼ ì§ì ‘ ì—°ê´€ëœ í‚¤ì›Œë“œ ì„ íƒ
4. ì¶”ìƒì  ê°œë…ë³´ë‹¤ êµ¬ì²´ì  ìš©ì–´ ìš°ì„ 

ë°˜ë“œì‹œ ë‹¤ìŒ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3", "í‚¤ì›Œë“œ4"]
"""
    
    try:
        response = model.generate_content(prompt)
        result_text = response.text.strip()
        
        match = re.search(r'\["[^"]*"(?:\s*,\s*"[^"]*")*\]', result_text)
        if match:
            keywords = json.loads(match.group())
            return keywords[:4]
    except Exception as e:
        print(f"ë‹¨ì¼ ë¬¸ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
    
    return ["ë²•ì•ˆ", "ê°œì •", "ì •ì±…", "ì‹œí–‰"]

# ===== ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ =====

def legal_specialized_processing_system():
    """ë²•ë¥  ë¬¸ì„œ íŠ¹í™” ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    print("ğŸš€ ë²•ë¥  ë¬¸ì„œ íŠ¹í™” ì²˜ë¦¬ ì‹œìŠ¤í…œ ì‹œì‘")
    start_time = time.time()
    
    # Gemini ëª¨ë¸ ìƒì„± (í´ëŸ¬ìŠ¤í„°ë§ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œìš©)
    model = genai.GenerativeModel('gemini-1.5-pro-latest')

    # ë°ì´í„° ë¡œë“œ
    print("ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")
    file_path = Path(r"C:/Users/1-02/Desktop/DAMF2/laws-radar/geovote/data/bill_filtered_final.csv")
    
    dtype_spec = {
        'age': 'int16',
        'bill_id': 'category'
    }
    
    df = pd.read_csv(file_path, dtype=dtype_spec, encoding='utf-8-sig')
    print(f"ğŸ“Š ì›ë³¸ ë°ì´í„°: {len(df)}ê°œ ì˜ì•ˆ")

    # ì›ë³¸ í…ìŠ¤íŠ¸ ë³„ë„ ë³´ê´€ (í´ëŸ¬ìŠ¤í„°ë§ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œìš©)
    df['original_content'] = df['content'].copy()

    # 1ë‹¨ê³„: ë²•ë¥  ë¬¸ì„œ íŠ¹í™” ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    print("ğŸ”„ ë²•ë¥  ë¬¸ì„œ íŠ¹í™” ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìˆ˜í–‰ ì¤‘...")
    df = improved_content_preprocessing(df)

    # ì „ì²˜ë¦¬ ê²°ê³¼ ê²€ì¦
    print("\nğŸ” ë²•ë¥  íŠ¹í™” ì „ì²˜ë¦¬ ê²°ê³¼ ê²€ì¦:")
    processed_count = (df['content'] != '').sum()
    print(f"   - ì²˜ë¦¬ëœ ë¬¸ì„œ: {processed_count}ê°œ")
    print(f"   - ë¹ˆ ë¬¸ì„œ: {len(df) - processed_count}ê°œ")
    
    # ìƒ˜í”Œ í™•ì¸ì„ ìœ„í•œ ë””ë²„ê¹… ì¶œë ¥
    if processed_count > 0:
        sample_idx = df[df['content'] != ''].index[0]
        print(f"   - ìƒ˜í”Œ ê²°ê³¼: {df.iloc[sample_idx]['content'][:200]}...")

    # 2ë‹¨ê³„: Gemini ì›ë³¸ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
    print("ğŸ¤– Gemini ì›ë³¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ ì¤‘...")
    clusters = gemini_clustering_from_original(
        df['original_content'].tolist(),
        df['title'].tolist(),
        model
    )
    
    # í´ëŸ¬ìŠ¤í„° ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ì— ì ìš©
    df['topic'] = df.index.map(lambda x: clusters.get(x, -1))

    # 3ë‹¨ê³„: Gemini ì›ë³¸ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
    print("ğŸ”„ Gemini ì›ë³¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
    topic_labels = {}
    
    # í´ëŸ¬ìŠ¤í„°ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
    unique_topics = df[df['topic'] != -1]['topic'].unique()
    
    def extract_cluster_keywords(cid):
        cluster_docs = df[df['topic'] == cid].index.tolist()
        
        try:
            keywords = gemini_extract_cluster_keywords(
                cluster_docs,
                df['original_content'].tolist(),
                df['title'].tolist(),
                model
            )
            return cid, keywords
            
        except Exception as e:
            print(f"í´ëŸ¬ìŠ¤í„° {cid} í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return cid, ["ë²•ì•ˆ", "ê°œì •", "ì •ì±…", "ì‹œí–‰"]
    
    # ë³‘ë ¬ ì²˜ë¦¬ë¡œ í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œ ì¶”ì¶œ
    with ThreadPoolExecutor(max_workers=6) as executor:
        futures = [executor.submit(extract_cluster_keywords, cid) for cid in unique_topics]
        
        for future in tqdm(as_completed(futures), total=len(futures), desc="í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œ ì¶”ì¶œ"):
            cid, keywords = future.result()
            topic_labels[cid] = keywords

    # ë‹¨ì¼ ë¬¸ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    single_docs = df[df['topic'] == -1]
    print(f"ğŸ”„ ë‹¨ì¼ ë¬¸ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘... ({len(single_docs)}ê°œ)")
    
    def extract_single_doc_keywords(idx_row):
        idx, row = idx_row
        try:
            keywords = gemini_extract_single_keywords(
                row['original_content'], 
                row['title'], 
                model
            )
            unique_topic_id = -1 * (idx + 2)
            return idx, unique_topic_id, keywords
        except Exception as e:
            print(f"ë‹¨ì¼ ë¬¸ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            unique_topic_id = -1 * (idx + 2)
            return idx, unique_topic_id, ["ë²•ì•ˆ", "ê°œì •", "ì •ì±…", "ì‹œí–‰"]
    
    if len(single_docs) > 0:
        with ThreadPoolExecutor(max_workers=6) as executor:
            futures = [executor.submit(extract_single_doc_keywords, (idx, row)) 
                      for idx, row in single_docs.iterrows()]
            
            for future in tqdm(as_completed(futures), total=len(futures), desc="ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬"):
                idx, unique_topic_id, keywords = future.result()
                topic_labels[unique_topic_id] = keywords
                df.at[idx, 'topic'] = unique_topic_id

    # topic_label ì»¬ëŸ¼ ìƒì„±
    df['topic_label'] = df['topic'].map(lambda x: ', '.join(topic_labels.get(x, [])))
    
    # ë¹ˆ í‚¤ì›Œë“œ ë¼ë²¨ ì²˜ë¦¬
    empty_labels = df[df['topic_label'] == '']
    if len(empty_labels) > 0:
        print(f"âš ï¸ ë¹ˆ í‚¤ì›Œë“œ ë¼ë²¨ {len(empty_labels)}ê°œ ë°œê²¬ - ê¸°ë³¸ í‚¤ì›Œë“œë¡œ ëŒ€ì²´")
        df.loc[df['topic_label'] == '', 'topic_label'] = 'ë²•ì•ˆ, ê°œì •, ì •ì±…, ì‹œí–‰'

    # original_content ì»¬ëŸ¼ ì œê±°
    df.drop('original_content', axis=1, inplace=True)

    print(f"âœ… ìµœì¢… ì²˜ë¦¬ëœ ì˜ì•ˆ: {len(df)}ê°œ")
    print(f"âœ… ìƒì„±ëœ í´ëŸ¬ìŠ¤í„°: {len(set(df['topic']))}ê°œ")

    # ê²°ê³¼ ì €ì¥
    output_path = Path('data/bill_legal_specialized_processing.csv')
    output_path.parent.mkdir(exist_ok=True)
    output_columns = ['age', 'title', 'bill_id', 'bill_number', 'content', 'topic', 'topic_label']
    df[output_columns].to_csv(output_path, index=False, encoding='utf-8-sig')
    
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")
    
    # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
    end_time = time.time()
    processing_time = end_time - start_time
    print(f"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ ({processing_time/60:.2f}ë¶„)")
    
    # ê²°ê³¼ ìš”ì•½
    print("\nğŸ“ˆ ë²•ë¥  ë¬¸ì„œ íŠ¹í™” ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½:")
    topic_summary = df.groupby('topic').agg({
        'title': 'count',
        'topic_label': 'first'
    }).rename(columns={'title': 'document_count'}).sort_values('document_count', ascending=False)
    
    print(f"   - ì´ í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(topic_summary)}ê°œ")
    print(f"   - ì´ ë¬¸ì„œ ìˆ˜: {len(df)}ê°œ")
    
    print(f"\nğŸ” ì£¼ìš” í´ëŸ¬ìŠ¤í„°ë³„ í‚¤ì›Œë“œ (ë²•ë¥  íŠ¹í™”):")
    for i, (topic_id, row) in enumerate(topic_summary.head(10).iterrows()):
        keywords = row['topic_label']
        print(f"   {i+1}. [{row['document_count']}ê°œ ë¬¸ì„œ] {keywords}")
    
    print(f"\nğŸ¯ ë²•ë¥  ë¬¸ì„œ íŠ¹í™” ì²˜ë¦¬ ë°©ì‹:")
    print(f"   - âœ… content ì²˜ë¦¬: ë²•ë¥  ë¬¸ì„œ íŠ¹í™” ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸")
    print(f"   - âœ… ë³µí•© ëª…ì‚¬ ë¶„ë¦¬: 'ì§€ë°©ì¶œì…êµ­Â·ì™¸êµ­ì¸ê´€ì„œ' â†’ 'ì§€ë°© ì¶œì…êµ­ ì™¸êµ­ì¸ ê´€ì„œ'")
    print(f"   - âœ… ë²•ë¥  ìš©ì–´ ë³´ì¡´: 50ê°œ ì´ìƒ ë²•ë¥  ì „ë¬¸ìš©ì–´ ë³´ì¡´")
    print(f"   - âœ… ë§¥ë½ ì¸ì‹ í•„í„°ë§: 3ë‹¨ê³„ ê³„ì¸µì  ë¶ˆìš©ì–´ ì œê±°")
    print(f"   - âœ… í´ëŸ¬ìŠ¤í„°ë§: Geminiê°€ ì›ë³¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ ìˆ˜í–‰")
    print(f"   - âœ… í‚¤ì›Œë“œ ì¶”ì¶œ: Geminiê°€ ì›ë³¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ ìˆ˜í–‰")
    
    return df, topic_labels

if __name__ == '__main__':
    # ë””ë ‰í† ë¦¬ ìƒì„±
    Path("data").mkdir(exist_ok=True)
    
    # ë²•ë¥  ë¬¸ì„œ íŠ¹í™” ì²˜ë¦¬ ì‹œìŠ¤í…œ ì‹¤í–‰
    df_result, topic_labels_result = legal_specialized_processing_system()
